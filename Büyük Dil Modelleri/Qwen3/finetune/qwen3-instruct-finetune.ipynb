{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kodun hazÄ±rlanmasÄ±nda unsloth dÃ¶kÃ¼mantasyonundan yararlanÄ±lmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-23T07:15:37.674006Z",
     "iopub.status.busy": "2025-10-23T07:15:37.673759Z",
     "iopub.status.idle": "2025-10-23T07:19:31.561298Z",
     "shell.execute_reply": "2025-10-23T07:19:31.560551Z",
     "shell.execute_reply.started": "2025-10-23T07:15:37.673982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.10.8-py3-none-any.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.10.9 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.10.9-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.1.0)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.1.1)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.9.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.16.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.0.0rc2)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.53.3)\n",
      "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.23.0,>=0.7.9 (from unsloth)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.19.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (0.28.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (0.21.2)\n",
      "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 (from unsloth)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.10.9->unsloth)\n",
      "  Downloading torchao-0.14.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.10.9->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.10.9->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.10.9->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.24.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (14.1.0)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.12.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.34.0->unsloth) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.34.0->unsloth) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.34.0->unsloth) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.34.0->unsloth) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface_hub>=0.34.0->unsloth) (8.3.0)\n",
      "Downloading unsloth-2025.10.8-py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m347.3/347.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.10.9-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.14.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, nvidia-cusparselt-cu12, triton, sympy, shtab, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface_hub, tyro, tokenizers, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 1.0.0rc2\n",
      "    Uninstalling huggingface-hub-1.0.0rc2:\n",
      "      Successfully uninstalled huggingface-hub-1.0.0rc2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.1 cut_cross_entropy-25.1.1 huggingface_hub-0.35.3 msgspec-0.19.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pyarrow-21.0.0 shtab-1.7.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 torchao-0.14.0 torchvision-0.23.0 transformers-4.56.2 triton-3.4.0 trl-0.23.0 tyro-0.9.35 unsloth-2025.10.8 unsloth_zoo-2025.10.9 xformers-0.0.32.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BÃ¼yÃ¼k Dil Modellerini olduÄŸu gibi eÄŸitmek iÃ§in uzun zaman, fazla kaynak ve bÃ¼yÃ¼k veriye ihtiyaÃ§ vardÄ±r. Bu sorunun Ã¶nÃ¼ne geÃ§mek iÃ§in bir takÄ±m kÃ¼tÃ¼phaneler ve yÃ¶ntemler kullanÄ±lÄ±r. KullanÄ±lan kÃ¼tÃ¼phanelerden birisi de unsloth kÃ¼tÃ¼phanesidir. Bu kÃ¼tÃ¼phane istenilen aÃ§Ä±k kaynaklÄ± bÃ¼yÃ¼k dil modellerini sisteme Ã§eker ve Ã§eÅŸitli yÃ¶ntemler sayesinde yÃ¼ksek GB gereken veriyi kÃ¼Ã§Ã¼lterek eÄŸitimin kolay gerÃ§ekleÅŸtirilmesini ve Ã§Ä±ktÄ±larÄ±n istenilen yerlerde saklanmasÄ±nÄ± saÄŸlar. Unsloth kÃ¼tÃ¼phanesinden kÄ±saca bahsettiÄŸimize gÃ¶re yÃ¶ntemlere deÄŸinelim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahsedilen ve kodta geÃ§en yÃ¶ntemlerden birisi de 4bit kuantizasyon tekniÄŸidir. Model aÄŸÄ±rlÄ±klarÄ± genellikle 32 bit float ya da 16 bit float olarak saklanÄ±r. BÃ¼yÃ¼k modellerde bu aÄŸÄ±rlÄ±klar sistemde Ã§ok fazla yer kaplar. 4 bit kuantizasyon yÃ¶ntemi ile aÄŸÄ±rlÄ±klar normalden 4 ile 8 kat daha az yer kaplar. Ancak saklanan verinin kesinliÄŸinin (precision) dÃ¼ÅŸÃ¼k 16 ve 32 bitliÄŸe gÃ¶re daha dÃ¼ÅŸÃ¼k olmasÄ±ndan dolayÄ± sonuÃ§lar teorik olarak biraz dÃ¼ÅŸÃ¼k Ã§Ä±kabilir. Ancak modern tekniklerle (Ã¶rn. Quantization-Aware Training, GPTQ) kayÄ±p minimuma indirilebilir.\n",
    "\n",
    "## Quantization-Aware Training (QAT)\n",
    "\n",
    "Modeli FP32 veya FP16 olarak eÄŸitirken aynÄ± zamanda aÄŸÄ±rlÄ±klarÄ±n kuantize edilmiÅŸ hallerini de simÃ¼le eder. EÄŸitim sÄ±rasÄ±nda aÄŸÄ±rlÄ±klar hem float hem de kuantize edilmiÅŸ halde bulunur. Bu yÃ¶ntemin amacÄ± kuantizasyon sonrasÄ± performans kaybÄ±nÄ± Ã¶nceden tahmin etmek ve minimize etmektir. Forward pass sÄ±rasÄ±nda model, kuantize edilmiÅŸ aÄŸÄ±rlÄ±klarÄ± kullanÄ±r. Backpropagation sÄ±rasÄ±nda gradientâ€™ler float aÄŸÄ±rlÄ±klar Ã¼zerinden hesaplanÄ±r. BÃ¶ylece model, kuantize edilmiÅŸ aÄŸÄ±rlÄ±klarÄ±n getirdiÄŸi hataya \"alÄ±ÅŸÄ±r\" ve performans kaybÄ± minimize edilir. EÄŸitim sÃ¼resi artar, Ã§Ã¼nkÃ¼ kuantizasyon simÃ¼lasyonu ek hesaplama gerektirir.\n",
    "\n",
    "\n",
    "## GPTQ (Gradient-based Post-Training Quantization)\n",
    "\n",
    "BÃ¼yÃ¼k modelleri eÄŸittikten sonra kuantize eder. Gradient kullanarak 4-bit veya 3-bit kuantizasyon sÄ±rasÄ±nda hatayÄ± minimize eder. Model eÄŸitildikten sonra aÄŸÄ±rlÄ±klar alÄ±nÄ±r. GPTQ, aÄŸÄ±rlÄ±klarÄ± grup veya blok bazlÄ± olarak kuantize eder.\n",
    "Kuantizasyon hatasÄ±nÄ± minimize etmek iÃ§in local gradientâ€™ler kullanÄ±lÄ±r (gradient descent ile optimize edilir, ama model yeniden eÄŸitilmez). SonuÃ§ olarak 4-bit veya 3-bit aÄŸÄ±rlÄ±klar, orijinal model performansÄ±na Ã§ok yakÄ±n bir doÄŸrulukla elde edilir. Modeli yeniden eÄŸitmek gerekmez. Post-training adÄ±mÄ±, Ã¶zellikle bÃ¼yÃ¼k modellerde zaman alabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:19:31.563269Z",
     "iopub.status.busy": "2025-10-23T07:19:31.563061Z",
     "iopub.status.idle": "2025-10-23T07:20:34.318759Z",
     "shell.execute_reply": "2025-10-23T07:20:34.317856Z",
     "shell.execute_reply.started": "2025-10-23T07:19:31.563250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 07:19:37.925145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761203978.191483      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761203978.319079      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.8: Fast Qwen3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c91b9938fc4560aee232106182aba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f897ceecdb47d2bf08d6155f4a2329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5337a29081a9449da739faceda042ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0871f5caf14f998d8cf4a6be1c3499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64917371753d486d8c458e714cce91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbd52c529334500b83dacf2a58945e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227f7b3243f24dd49346c840619ec5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f63fc61d104b278e168d76a9c29b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c842b1467f3d49bc96593ed7708b7587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model iÅŸlemleri iÃ§in unsloth kÃ¼tÃ¼phanesi kullanÄ±lÄ±r. \n",
    "from unsloth import FastLanguageModel\n",
    "#Instruct modellerde bir chat template oluÅŸturmak iÃ§in bu kÃ¼tÃ¼phane kullanÄ±lÄ±r.\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "# AÃ§Ä±k kaynak LLM'lerin Ã§oÄŸu torch kullanÄ±r.\n",
    "import torch\n",
    "# Huggingface datasetlerini kullanmak iÃ§in datasets kÃ¼tÃ¼phanesi kullanÄ±lÄ±r.\n",
    "from datasets import load_dataset\n",
    "# Model eÄŸtimi iÃ§in\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/Qwen3-4B-Thinking-2507',\n",
    "    load_in_4bit = True,'\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:20:34.322626Z",
     "iopub.status.busy": "2025-10-23T07:20:34.322104Z",
     "iopub.status.idle": "2025-10-23T07:20:42.103446Z",
     "shell.execute_reply": "2025-10-23T07:20:42.102789Z",
     "shell.execute_reply.started": "2025-10-23T07:20:34.322596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Rank parametresidir. =! 0 olmalÄ±dÄ±r. 8, 16, 32, 64, 128 deÄŸerleri Ã¶nerilir.\n",
    "    # Bu deÄŸer arttÄ±kÃ§a model doÄŸruluÄŸu artar ancak overfitting riski de artar.\n",
    "    # EÄŸitilecek katmanlar\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    #lora parametreleri ve bias\n",
    "    # Ã¶zellikle lora da A B matrisinin katsayÄ±sÄ± olarak lora_alpha / rank seÃ§ilir.\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    # Uzun contextlerde True ya da unsloth ifadesi kullanÄ±lÄ±r.\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    #rank stabilized lora aktif mi pasif mi olacak onu belirler.\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:20:42.106078Z",
     "iopub.status.busy": "2025-10-23T07:20:42.105646Z",
     "iopub.status.idle": "2025-10-23T07:20:42.124022Z",
     "shell.execute_reply": "2025-10-23T07:20:42.123293Z",
     "shell.execute_reply.started": "2025-10-23T07:20:42.106059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (2-3): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (5-6): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (7-26): 20 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (27): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (28-33): 6 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (34): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (35): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:20:42.124995Z",
     "iopub.status.busy": "2025-10-23T07:20:42.124750Z",
     "iopub.status.idle": "2025-10-23T07:20:59.209907Z",
     "shell.execute_reply": "2025-10-23T07:20:59.209345Z",
     "shell.execute_reply.started": "2025-10-23T07:20:42.124971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6200dc3667da42a1aad1f21112255470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c19cea3cb14c7b9f8bdc0771573786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00005.parquet:   0%|          | 0.00/199M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e64d684a4c4b329b957e602b085b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00005.parquet:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c098ce7f2f4b64a2d347efff3dd22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00005.parquet:   0%|          | 0.00/203M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae368df1568d41bbbd78e3b93dc02adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00005.parquet:   0%|          | 0.00/200M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f43bdca026e4b8387c61aeab32ead99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00005.parquet:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2753a655e046bba4a83af91f00a063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/89120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'problem', 'solution', 'messages', 'system', 'conversations', 'generated_token_count', 'correct'],\n",
       "        num_rows: 89120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"open-r1/OpenThoughts-114k-math\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:20:59.210898Z",
     "iopub.status.busy": "2025-10-23T07:20:59.210644Z",
     "iopub.status.idle": "2025-10-23T07:20:59.217325Z",
     "shell.execute_reply": "2025-10-23T07:20:59.216822Z",
     "shell.execute_reply.started": "2025-10-23T07:20:59.210869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = ds['train'].select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:20:59.218553Z",
     "iopub.status.busy": "2025-10-23T07:20:59.218280Z",
     "iopub.status.idle": "2025-10-23T07:21:00.679609Z",
     "shell.execute_reply": "2025-10-23T07:21:00.678715Z",
     "shell.execute_reply.started": "2025-10-23T07:20:59.218536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen3-thinking\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:21:00.680706Z",
     "iopub.status.busy": "2025-10-23T07:21:00.680429Z",
     "iopub.status.idle": "2025-10-23T07:21:01.580725Z",
     "shell.execute_reply": "2025-10-23T07:21:01.579935Z",
     "shell.execute_reply.started": "2025-10-23T07:21:00.680682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_prompts(batch):\n",
    "    convs = batch[\"conversations\"]\n",
    "\n",
    "    texts = []\n",
    "    for conv in convs:\n",
    "        # \"from\" -> \"role\", \"value\" -> \"content\"\n",
    "        chat = [\n",
    "            {\"role\": msg[\"from\"], \"content\": msg[\"value\"]}\n",
    "            for msg in conv\n",
    "        ]\n",
    "\n",
    "        # Åimdi Qwen3 chat template'ine uygun\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:21:01.582268Z",
     "iopub.status.busy": "2025-10-23T07:21:01.581980Z",
     "iopub.status.idle": "2025-10-23T07:21:06.138403Z",
     "shell.execute_reply": "2025-10-23T07:21:06.137548Z",
     "shell.execute_reply.started": "2025-10-23T07:21:01.582243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cf5cf7169b44b58229de81c8934da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(format_prompts, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:21:06.139834Z",
     "iopub.status.busy": "2025-10-23T07:21:06.139534Z",
     "iopub.status.idle": "2025-10-23T07:21:06.147118Z",
     "shell.execute_reply": "2025-10-23T07:21:06.146295Z",
     "shell.execute_reply.started": "2025-10-23T07:21:06.139815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nReturn your final response within \\\\boxed{}. Let \\\\( a, b, c \\\\) be positive real numbers. Prove that\\n\\n$$\\n\\\\frac{1}{a(1+b)}+\\\\frac{1}{b(1+c)}+\\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc},\\n$$\\n\\nand that equality occurs if and only if \\\\( a = b = c = 1 \\\\).<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n<|begin_of_thought|>\\n\\nOkay, so I need to prove this inequality: \\\\(\\\\frac{1}{a(1+b)}+\\\\frac{1}{b(1+c)}+\\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc}\\\\), where \\\\(a, b, c\\\\) are positive real numbers, and equality holds if and only if \\\\(a = b = c = 1\\\\). Hmm, let me try to think through this step by step.\\n\\nFirst, I remember that inequalities often involve techniques like AM-GM, Cauchy-Schwarz, or other classical inequalities. Maybe I can start by looking at each term on the left side and see if I can manipulate them to relate to the right side.\\n\\nThe left side has three terms: \\\\(\\\\frac{1}{a(1+b)}\\\\), \\\\(\\\\frac{1}{b(1+c)}\\\\), and \\\\(\\\\frac{1}{c(1+a)}\\\\). Each denominator has a variable multiplied by \\\\(1\\\\) plus another variable. The right side is \\\\(\\\\frac{3}{1+abc}\\\\), which is symmetric in \\\\(a, b, c\\\\). Since the equality case is when all variables are 1, maybe substituting \\\\(a = b = c = 1\\\\) gives both sides equal to \\\\(3/(1+1) = 1.5\\\\), and the left side becomes \\\\(1/(1*(1+1))\\\\) three times, which is \\\\(0.5*3 = 1.5\\\\). So that checks out. But how do I get from here to the general proof?\\n\\nLet me consider using the AM-GM inequality. AM-GM states that for non-negative real numbers, the arithmetic mean is at least the geometric mean. Maybe I can apply AM-GM to the denominators or somehow to the entire terms. Alternatively, maybe Cauchy-Schwarz can be applied here. Let me think about Cauchy-Schwarz.\\n\\nCauchy-Schwarz in the form \\\\(\\\\left(\\\\sum \\\\frac{1}{x_i}\\\\right) \\\\left(\\\\sum x_i\\\\right) \\\\geq (n)^2\\\\). But I\\'m not sure if that\\'s directly applicable here. Alternatively, perhaps using the Titu\\'s lemma, which is a specific case of Cauchy-Schwarz: \\\\(\\\\sum \\\\frac{y_i^2}{x_i} \\\\geq \\\\frac{(\\\\sum y_i)^2}{\\\\sum x_i}\\\\). But again, not sure how to fit the terms here.\\n\\nWait, maybe another approach. Let\\'s consider the substitution \\\\(x = 1/a\\\\), \\\\(y = 1/b\\\\), \\\\(z = 1/c\\\\). Then \\\\(a = 1/x\\\\), \\\\(b = 1/y\\\\), \\\\(c = 1/z\\\\), and the left side becomes:\\n\\n\\\\[\\n\\\\frac{1}{(1/x)(1 + 1/y)} + \\\\frac{1}{(1/y)(1 + 1/z)} + \\\\frac{1}{(1/z)(1 + 1/x)} = \\\\frac{x}{1 + 1/y} + \\\\frac{y}{1 + 1/z} + \\\\frac{z}{1 + 1/x}\\n\\\\]\\n\\nSimplify denominators:\\n\\n\\\\[\\n\\\\frac{x}{(y + 1)/y} + \\\\frac{y}{(z + 1)/z} + \\\\frac{z}{(x + 1)/x} = \\\\frac{xy}{y + 1} + \\\\frac{yz}{z + 1} + \\\\frac{zx}{x + 1}\\n\\\\]\\n\\nSo the left side becomes \\\\(\\\\frac{xy}{y + 1} + \\\\frac{yz}{z + 1} + \\\\frac{zx}{x + 1}\\\\), and we need to show that this is at least \\\\(3/(1 + (1/x)(1/y)(1/z)) = 3/(1 + 1/(xyz)) = \\\\frac{3xyz}{1 + xyz}\\\\). Wait, maybe this substitution complicates things more. Let me check:\\n\\nSo the inequality becomes:\\n\\n\\\\[\\n\\\\frac{xy}{y + 1} + \\\\frac{yz}{z + 1} + \\\\frac{zx}{x + 1} \\\\geq \\\\frac{3xyz}{1 + xyz}\\n\\\\]\\n\\nNot sure if that\\'s helpful. Maybe I should try another substitution or approach.\\n\\nAlternatively, let\\'s consider homogenization. The inequality is not homogeneous because the denominators on the left have degree 1 (since \\\\(a(1 + b)\\\\) is degree 1 if we consider variables \\\\(a, b, c\\\\)), while the right side denominator \\\\(1 + abc\\\\) is degree 3. So homogenizing might be a way. To do that, we need to make both sides have the same degree.\\n\\nLet me check degrees:\\n\\nLeft side terms: \\\\(\\\\frac{1}{a(1 + b)}\\\\). The denominator is \\\\(a + ab\\\\), so the term is \\\\(1/(a + ab)\\\\). If we consider variables as degree 1, then the denominator is degree 1 + 1 = 2, so each term is degree -2. So the entire left side is a sum of degree -2 terms.\\n\\nThe right side is \\\\(3/(1 + abc)\\\\). The denominator is 1 + abc, which is degree 3. So the right side is \\\\(3\\\\) divided by degree 3, so degree -3. Therefore, the inequality is comparing terms of degree -2 on the left with a term of degree -3 on the right. To homogenize, we need to introduce a common degree.\\n\\nSuppose we multiply both sides by \\\\(1 + abc\\\\). Then the inequality becomes:\\n\\n\\\\[\\n\\\\left(\\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)}\\\\right)(1 + abc) \\\\geq 3\\n\\\\]\\n\\nBut this still might not be homogeneous. Let me check the degrees after multiplication. The left side terms are each of degree -2, multiplied by \\\\(1 + abc\\\\) (degree 3), so each term becomes degree 1. So the left side becomes a sum of three terms each of degree 1, and the right side is 3 (degree 0). Wait, that doesn\\'t make sense. Maybe homogenization isn\\'t straightforward here.\\n\\nAlternatively, perhaps we can use substitution \\\\(abc = k\\\\). But not sure yet.\\n\\nAnother idea: maybe apply the Cauchy-Schwarz inequality on the left-hand side. Let\\'s try that.\\n\\nThe left-hand side is \\\\(\\\\sum \\\\frac{1}{a(1 + b)}\\\\). Let me consider each term as \\\\(\\\\frac{1}{a(1 + b)} = \\\\frac{1}{a + ab}\\\\). So the sum is \\\\(\\\\sum \\\\frac{1}{a + ab}\\\\).\\n\\nAlternatively, maybe use substitution variables. Let me set \\\\(x = a\\\\), \\\\(y = b\\\\), \\\\(z = c\\\\). Not sure.\\n\\nWait, perhaps the condition for equality is when \\\\(a = b = c = 1\\\\). So maybe using substitution \\\\(a = x\\\\), \\\\(b = y\\\\), \\\\(c = z\\\\), but not helpful.\\n\\nAlternatively, perhaps set \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), which sometimes helps in cyclic inequalities, but not sure here.\\n\\nAlternatively, let\\'s try to use AM-GM on the denominators. For example, \\\\(1 + b \\\\geq 2\\\\sqrt{b}\\\\) by AM-GM. Then \\\\(a(1 + b) \\\\geq a \\\\cdot 2\\\\sqrt{b}\\\\), so \\\\(\\\\frac{1}{a(1 + b)} \\\\leq \\\\frac{1}{2a\\\\sqrt{b}}\\\\). But this gives an upper bound, whereas we need a lower bound. So this approach might not work directly.\\n\\nAlternatively, reverse the inequality. If we can find a lower bound for each term \\\\(\\\\frac{1}{a(1 + b)}\\\\), but since AM-GM gives an upper bound here, maybe not useful.\\n\\nAlternatively, consider the following substitution: let \\\\(abc = k\\\\). Since the right-hand side is \\\\(3/(1 + k)\\\\), perhaps we can relate the left-hand side in terms of \\\\(k\\\\). But how?\\n\\nAlternatively, let\\'s use substitution \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), which makes \\\\(abc = 1\\\\). Wait, but this substitution only covers the case when \\\\(abc = 1\\\\), which is a specific case, not general. Maybe not helpful.\\n\\nWait, but if I assume \\\\(abc = 1\\\\), then the inequality becomes \\\\(\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{3}{2}\\\\). Let\\'s check if that\\'s true when \\\\(abc = 1\\\\). If \\\\(a = b = c = 1\\\\), yes, equality. But maybe we can use substitution to reduce the general case to the case \\\\(abc = 1\\\\).\\n\\nThat\\'s a common technique. Let me try that. Suppose we let \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\). Then \\\\(abc = 1\\\\), and substituting into the inequality:\\n\\nLeft side becomes:\\n\\n\\\\[\\n\\\\frac{1}{(x/y)(1 + y/z)} + \\\\frac{1}{(y/z)(1 + z/x)} + \\\\frac{1}{(z/x)(1 + x/y)}\\n\\\\]\\n\\nSimplify each term:\\n\\nFirst term: \\\\(\\\\frac{1}{(x/y)(1 + y/z)} = \\\\frac{y}{x} \\\\cdot \\\\frac{1}{1 + y/z} = \\\\frac{y}{x} \\\\cdot \\\\frac{z}{z + y} = \\\\frac{yz}{x(z + y)}\\\\)\\n\\nSecond term: Similarly, \\\\(\\\\frac{1}{(y/z)(1 + z/x)} = \\\\frac{z}{y} \\\\cdot \\\\frac{x}{x + z} = \\\\frac{zx}{y(x + z)}\\\\)\\n\\nThird term: \\\\(\\\\frac{1}{(z/x)(1 + x/y)} = \\\\frac{x}{z} \\\\cdot \\\\frac{y}{y + x} = \\\\frac{xy}{z(x + y)}\\\\)\\n\\nSo the left side is:\\n\\n\\\\[\\n\\\\frac{yz}{x(y + z)} + \\\\frac{zx}{y(z + x)} + \\\\frac{xy}{z(x + y)}\\n\\\\]\\n\\nAnd the right side is \\\\(\\\\frac{3}{1 + 1} = \\\\frac{3}{2}\\\\). So the inequality reduces to:\\n\\n\\\\[\\n\\\\frac{yz}{x(y + z)} + \\\\frac{zx}{y(z + x)} + \\\\frac{xy}{z(x + y)} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nHmm, this seems like a known inequality. Maybe Nesbitt\\'s inequality? Nesbitt\\'s inequality states that for positive real numbers \\\\(a, b, c\\\\),\\n\\n\\\\[\\n\\\\frac{a}{b + c} + \\\\frac{b}{a + c} + \\\\frac{c}{a + b} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nBut in our case, the terms are \\\\(\\\\frac{yz}{x(y + z)}\\\\), which can be written as \\\\(\\\\frac{yz}{x(y + z)} = \\\\frac{1}{x} \\\\cdot \\\\frac{yz}{y + z}\\\\). So if we let \\\\(a = yz\\\\), \\\\(b = zx\\\\), \\\\(c = xy\\\\), then maybe we can relate it to Nesbitt\\'s. Let me try:\\n\\nLet \\\\(a = yz\\\\), \\\\(b = zx\\\\), \\\\(c = xy\\\\). Then the left side becomes:\\n\\n\\\\[\\n\\\\frac{a}{x(y + z)} + \\\\frac{b}{y(z + x)} + \\\\frac{c}{z(x + y)}\\n\\\\]\\n\\nBut \\\\(x = \\\\sqrt{\\\\frac{bc}{a}}\\\\), \\\\(y = \\\\sqrt{\\\\frac{ac}{b}}\\\\), \\\\(z = \\\\sqrt{\\\\frac{ab}{c}}\\\\), which seems complicated. Maybe not helpful.\\n\\nAlternatively, note that \\\\( \\\\frac{yz}{x(y + z)} = \\\\frac{yz}{x(y + z)} \\\\). If I consider variables \\\\(x, y, z\\\\), maybe using AM-GM on each term. Let me think.\\n\\nAlternatively, use Cauchy-Schwarz on the left side terms. For example,\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} = \\\\sum \\\\frac{y^2 z^2}{x y z (y + z)} = \\\\sum \\\\frac{y z}{x (y + z)}\\n\\\\]\\n\\nWait, not helpful. Alternatively, write each term as \\\\(\\\\frac{z}{x(y + z)/y}\\\\), which might not be useful.\\n\\nAlternatively, apply Cauchy-Schwarz in the following form:\\n\\n\\\\[\\n\\\\left( \\\\sum \\\\frac{yz}{x(y + z)} \\\\right) \\\\left( \\\\sum x(y + z) \\\\right) \\\\geq (yz + zx + xy)^2\\n\\\\]\\n\\nThis is similar to Titu\\'s lemma. Let me verify:\\n\\nUsing Cauchy-Schwarz,\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{( \\\\sqrt{yz} + \\\\sqrt{zx} + \\\\sqrt{xy} )^2 }{ \\\\sum x(y + z) }\\n\\\\]\\n\\nBut I\\'m not sure if this is directly applicable. Wait, Titu\\'s lemma is:\\n\\n\\\\[\\n\\\\sum \\\\frac{a_i^2}{b_i} \\\\geq \\\\frac{(\\\\sum a_i)^2}{\\\\sum b_i}\\n\\\\]\\n\\nSo if we set \\\\(a_i = \\\\sqrt{yz}\\\\), \\\\(b_i = x(y + z)\\\\), then:\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} = \\\\sum \\\\frac{(\\\\sqrt{yz})^2}{x(y + z)} \\\\geq \\\\frac{(\\\\sqrt{yz} + \\\\sqrt{zx} + \\\\sqrt{xy})^2}{\\\\sum x(y + z)}\\n\\\\]\\n\\nCompute numerator: \\\\((\\\\sqrt{yz} + \\\\sqrt{zx} + \\\\sqrt{xy})^2 = yz + zx + xy + 2(\\\\sqrt{y^2 z x} + \\\\sqrt{z^2 x y} + \\\\sqrt{x^2 y z})\\\\)\\n\\nWait, but that seems messy. The denominator is \\\\(\\\\sum x(y + z) = x(y + z) + y(z + x) + z(x + y) = 2(xy + yz + zx)\\\\). So denominator is \\\\(2(xy + yz + zx)\\\\). Therefore, the inequality becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{xy + yz + zx + 2(\\\\sqrt{x y^2 z} + \\\\sqrt{x z^2 y} + \\\\sqrt{y x^2 z})}{2(xy + yz + zx)}\\n\\\\]\\n\\nHmm, not sure if this helps. The numerator inside the fraction is \\\\(xy + yz + zx + 2(\\\\sqrt{x y z}(\\\\sqrt{y} + \\\\sqrt{z} + \\\\sqrt{x}))\\\\). Unless there\\'s a relation here, this might not lead to the desired \\\\(\\\\frac{3}{2}\\\\).\\n\\nAlternatively, maybe using the AM-HM inequality. For each term \\\\(\\\\frac{yz}{x(y + z)}\\\\), note that \\\\(y + z \\\\geq 2\\\\sqrt{yz}\\\\), so:\\n\\n\\\\[\\n\\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{yz}{x \\\\cdot 2\\\\sqrt{yz}} = \\\\frac{\\\\sqrt{yz}}{2x}\\n\\\\]\\n\\nThen the sum becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\sum \\\\frac{\\\\sqrt{yz}}{2x} = \\\\frac{1}{2} \\\\left( \\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\right)\\n\\\\]\\n\\nBut again, not sure how to proceed from here. The right-hand side is \\\\(\\\\frac{1}{2}\\\\) times something. If we can show that \\\\(\\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\geq 3\\\\), then we would get the lower bound of \\\\(3/2\\\\), which is exactly what we need. But is this true?\\n\\nWait, by AM-GM,\\n\\n\\\\[\\n\\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\geq 3 \\\\sqrt[3]{\\\\frac{\\\\sqrt{yz}}{x} \\\\cdot \\\\frac{\\\\sqrt{zx}}{y} \\\\cdot \\\\frac{\\\\sqrt{xy}}{z}}\\n\\\\]\\n\\nCompute the cube root:\\n\\nInside the cube root:\\n\\n\\\\[\\n\\\\frac{\\\\sqrt{yz} \\\\cdot \\\\sqrt{zx} \\\\cdot \\\\sqrt{xy}}{x y z} = \\\\frac{(yz)^{1/2} (zx)^{1/2} (xy)^{1/2}}{x y z} = \\\\frac{(y z z x x y)^{1/2}}{x y z} = \\\\frac{(x^2 y^2 z^2)^{1/2}}{x y z} = \\\\frac{x y z}{x y z} = 1\\n\\\\]\\n\\nTherefore,\\n\\n\\\\[\\n\\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\geq 3 \\\\sqrt[3]{1} = 3\\n\\\\]\\n\\nHence,\\n\\n\\\\[\\n\\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{1}{2} \\\\times 3 = \\\\frac{3}{2}\\n\\\\]\\n\\nWhich is exactly what we needed! Therefore, when \\\\(abc = 1\\\\), the original inequality holds with equality when \\\\(a = b = c = 1\\\\). But wait, does this substitution cover all cases? Because we assumed \\\\(abc = 1\\\\), but the original problem is for any positive real numbers \\\\(a, b, c\\\\). So how does this help?\\n\\nAh, right, perhaps we can use a substitution to normalize \\\\(abc = 1\\\\). Let me see. Suppose we set \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), which gives \\\\(abc = 1\\\\). Then the inequality reduces to the case above, which we proved is \\\\(\\\\geq \\\\frac{3}{2}\\\\). But the original inequality\\'s right-hand side when \\\\(abc = 1\\\\) is indeed \\\\(3/(1 + 1) = 3/2\\\\). So this substitution works for the case \\\\(abc = 1\\\\), but what about the general case?\\n\\nWait, actually, if we can show that the inequality is equivalent under scaling. Let\\'s consider scaling variables \\\\(a, b, c\\\\) by some factor. Suppose we set \\\\(a = kx\\\\), \\\\(b = ky\\\\), \\\\(c = kz\\\\), such that \\\\(abc = (k x)(k y)(k z) = k^3 xyz\\\\). Maybe choose \\\\(k\\\\) such that \\\\(k^3 xyz = 1\\\\), but not sure.\\n\\nAlternatively, since the inequality is not homogeneous, perhaps we can use substitution variables to make it homogeneous. Let me check the original inequality again:\\n\\n\\\\[\\n\\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nLet me multiply both sides by \\\\(1 + abc\\\\):\\n\\n\\\\[\\n\\\\left( \\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)} \\\\right)(1 + abc) \\\\geq 3\\n\\\\]\\n\\nNow, expanding the left-hand side:\\n\\nEach term would be \\\\(\\\\frac{1}{a(1 + b)} \\\\cdot 1 + \\\\frac{1}{a(1 + b)} \\\\cdot abc\\\\), similarly for the other terms. Let\\'s compute this:\\n\\nFirst term: \\\\(\\\\frac{1}{a(1 + b)} + \\\\frac{abc}{a(1 + b)} = \\\\frac{1 + bc}{a(1 + b)}\\\\)\\n\\nSimilarly, second term: \\\\(\\\\frac{1}{b(1 + c)} + \\\\frac{abc}{b(1 + c)} = \\\\frac{1 + ac}{b(1 + c)}\\\\)\\n\\nThird term: \\\\(\\\\frac{1}{c(1 + a)} + \\\\frac{abc}{c(1 + a)} = \\\\frac{1 + ab}{c(1 + a)}\\\\)\\n\\nTherefore, the left-hand side becomes:\\n\\n\\\\[\\n\\\\frac{1 + bc}{a(1 + b)} + \\\\frac{1 + ac}{b(1 + c)} + \\\\frac{1 + ab}{c(1 + a)}\\n\\\\]\\n\\nSo we need to show:\\n\\n\\\\[\\n\\\\frac{1 + bc}{a(1 + b)} + \\\\frac{1 + ac}{b(1 + c)} + \\\\frac{1 + ab}{c(1 + a)} \\\\geq 3\\n\\\\]\\n\\nHmm, interesting. Now, this seems more manageable. Let me try to simplify each term.\\n\\nTake the first term: \\\\(\\\\frac{1 + bc}{a(1 + b)}\\\\). Let\\'s split the numerator:\\n\\n\\\\[\\n\\\\frac{1}{a(1 + b)} + \\\\frac{bc}{a(1 + b)} = \\\\frac{1}{a(1 + b)} + \\\\frac{bc}{a(1 + b)}\\n\\\\]\\n\\nWait, that\\'s redundant. Alternatively, maybe manipulate the term:\\n\\n\\\\[\\n\\\\frac{1 + bc}{a(1 + b)} = \\\\frac{1}{a(1 + b)} + \\\\frac{bc}{a(1 + b)} = \\\\frac{1}{a(1 + b)} + \\\\frac{bc}{a(1 + b)}\\n\\\\]\\n\\nNot helpful. Wait, maybe factor \\\\(1 + bc\\\\) as \\\\(1 + b \\\\cdot c\\\\), but not obvious.\\n\\nAlternatively, note that \\\\(1 + bc = (1 + b) + bc - b = (1 + b) + b(c - 1)\\\\). Hmm, not sure.\\n\\nWait, maybe use substitutions. Let me set \\\\(x = 1 + a\\\\), \\\\(y = 1 + b\\\\), \\\\(z = 1 + c\\\\). Then \\\\(a = x - 1\\\\), \\\\(b = y - 1\\\\), \\\\(c = z - 1\\\\). But this might complicate the expressions.\\n\\nAlternatively, note that in the equality case \\\\(a = b = c = 1\\\\), each term becomes \\\\(\\\\frac{1 + 1*1}{1*(1 + 1)} = \\\\frac{2}{2} = 1\\\\), so the left-hand side is 3, which matches the right-hand side. So equality holds here. Now, how to show that each term contributes at least 1?\\n\\nWait, no, the left-hand side is the sum of three terms, each of which might not be 1, but their sum is at least 3. So maybe using AM-GM on each term.\\n\\nWait, consider the term \\\\(\\\\frac{1 + bc}{a(1 + b)}\\\\). Let me write this as \\\\(\\\\frac{1}{a(1 + b)} + \\\\frac{bc}{a(1 + b)}\\\\). The first part is \\\\(\\\\frac{1}{a(1 + b)}\\\\), which is part of the original left-hand side. The second part is \\\\(\\\\frac{bc}{a(1 + b)}\\\\). Maybe there\\'s a relation between these terms.\\n\\nAlternatively, use AM-GM on \\\\(1 + bc \\\\geq 2\\\\sqrt{bc}\\\\). Then:\\n\\n\\\\[\\n\\\\frac{1 + bc}{a(1 + b)} \\\\geq \\\\frac{2\\\\sqrt{bc}}{a(1 + b)}\\n\\\\]\\n\\nBut not sure how to sum these up.\\n\\nAlternatively, maybe make substitutions to symmetrize the terms. Let me consider the term \\\\(\\\\frac{1 + bc}{a(1 + b)}\\\\). Let me write \\\\(1 + bc = bc + 1\\\\). Then:\\n\\n\\\\[\\n\\\\frac{bc + 1}{a(1 + b)} = \\\\frac{bc}{a(1 + b)} + \\\\frac{1}{a(1 + b)}\\n\\\\]\\n\\nBut this just splits the term into two parts again. Not helpful.\\n\\nWait, maybe consider the following. Let me set \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\). Then \\\\(abc = 1\\\\), as before. Then:\\n\\nThe term \\\\(\\\\frac{1 + bc}{a(1 + b)}\\\\) becomes:\\n\\n\\\\[\\n\\\\frac{1 + \\\\frac{y}{z} \\\\cdot \\\\frac{z}{x}}{\\\\frac{x}{y} \\\\left(1 + \\\\frac{y}{z}\\\\right)} = \\\\frac{1 + \\\\frac{y}{x}}{\\\\frac{x}{y} \\\\left( \\\\frac{z + y}{z} \\\\right)} = \\\\frac{\\\\frac{x + y}{x}}{\\\\frac{x(z + y)}{y z}} = \\\\frac{(x + y) y z}{x^2 (z + y)} = \\\\frac{y z}{x^2}\\n\\\\]\\n\\nWait, that seems complicated. Let me compute step by step:\\n\\nFirst, \\\\(bc = \\\\frac{y}{z} \\\\cdot \\\\frac{z}{x} = \\\\frac{y}{x}\\\\), so \\\\(1 + bc = 1 + \\\\frac{y}{x} = \\\\frac{x + y}{x}\\\\).\\n\\nDenominator: \\\\(a(1 + b) = \\\\frac{x}{y} \\\\left(1 + \\\\frac{y}{z}\\\\right) = \\\\frac{x}{y} \\\\cdot \\\\frac{z + y}{z} = \\\\frac{x(z + y)}{y z}\\\\).\\n\\nSo the entire term is \\\\(\\\\frac{(x + y)/x}{x(z + y)/(y z)} = \\\\frac{(x + y) y z}{x^2 (z + y)} = \\\\frac{y z}{x^2}\\\\).\\n\\nSimilarly, the other terms:\\n\\nSecond term: \\\\(\\\\frac{1 + ac}{b(1 + c)} = \\\\frac{1 + \\\\frac{x}{y} \\\\cdot \\\\frac{z}{x}}{\\\\frac{y}{z} (1 + \\\\frac{z}{x})} = \\\\frac{1 + \\\\frac{z}{y}}{\\\\frac{y}{z} \\\\cdot \\\\frac{x + z}{x}} = \\\\frac{(y + z)/y}{(y(x + z))/(z x)} = \\\\frac{(y + z) z x}{y^2 (x + z)} = \\\\frac{z x}{y^2}\\\\).\\n\\nThird term: \\\\(\\\\frac{1 + ab}{c(1 + a)} = \\\\frac{1 + \\\\frac{x}{y} \\\\cdot \\\\frac{y}{z}}{\\\\frac{z}{x} (1 + \\\\frac{x}{y})} = \\\\frac{1 + \\\\frac{x}{z}}{\\\\frac{z}{x} \\\\cdot \\\\frac{y + x}{y}} = \\\\frac{(z + x)/z}{(z(y + x))/(x y)} = \\\\frac{(z + x) x y}{z^2 (y + x)} = \\\\frac{x y}{z^2}\\\\).\\n\\nTherefore, the left-hand side becomes:\\n\\n\\\\[\\n\\\\frac{y z}{x^2} + \\\\frac{z x}{y^2} + \\\\frac{x y}{z^2}\\n\\\\]\\n\\nAnd we need to show that this sum is at least 3. But since \\\\(x, y, z\\\\) are positive real numbers, and by AM-GM:\\n\\n\\\\[\\n\\\\frac{y z}{x^2} + \\\\frac{z x}{y^2} + \\\\frac{x y}{z^2} \\\\geq 3 \\\\sqrt[3]{\\\\frac{y z}{x^2} \\\\cdot \\\\frac{z x}{y^2} \\\\cdot \\\\frac{x y}{z^2}} = 3 \\\\sqrt[3]{\\\\frac{y z \\\\cdot z x \\\\cdot x y}{x^2 y^2 z^2}} = 3 \\\\sqrt[3]{\\\\frac{x^2 y^2 z^2}{x^2 y^2 z^2}} = 3\\n\\\\]\\n\\nTherefore, the sum is at least 3, which is exactly what we needed. Therefore, the inequality holds when \\\\(abc = 1\\\\), and equality occurs when all terms are equal, i.e., \\\\(\\\\frac{y z}{x^2} = \\\\frac{z x}{y^2} = \\\\frac{x y}{z^2}\\\\). This implies \\\\(x = y = z\\\\), which translates back to \\\\(a = b = c = 1\\\\).\\n\\nBut this was under the substitution \\\\(abc = 1\\\\). So how do we generalize this to all positive real numbers \\\\(a, b, c\\\\)?\\n\\nWait, perhaps we can use the substitution where we set \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), but then this forces \\\\(abc = 1\\\\). However, in the original problem, \\\\(abc\\\\) can be any positive real number. So, how does this approach generalize?\\n\\nAlternatively, perhaps we can use a scaling argument. Let me assume that for any \\\\(a, b, c > 0\\\\), we can scale them to \\\\(a\\' = a t\\\\), \\\\(b\\' = b t\\\\), \\\\(c\\' = c t\\\\) such that \\\\(a\\' b\\' c\\' = 1\\\\). Wait, if we set \\\\(t = \\\\frac{1}{(abc)^{1/3}}\\\\), then \\\\(a\\' b\\' c\\' = (a t)(b t)(c t) = abc t^3 = abc \\\\cdot \\\\frac{1}{abc} = 1\\\\). So scaling variables by \\\\(t = \\\\frac{1}{(abc)^{1/3}}\\\\) makes \\\\(a\\' b\\' c\\' = 1\\\\).\\n\\nLet me apply this scaling. Let \\\\(t = \\\\frac{1}{(abc)^{1/3}}\\\\), so \\\\(a\\' = a t\\\\), \\\\(b\\' = b t\\\\), \\\\(c\\' = c t\\\\). Then \\\\(a\\' b\\' c\\' = 1\\\\). Substitute into the original inequality:\\n\\nLeft side:\\n\\n\\\\[\\n\\\\frac{1}{a\\'(1 + b\\')} + \\\\frac{1}{b\\'(1 + c\\')} + \\\\frac{1}{c\\'(1 + a\\')} = \\\\frac{1}{a t (1 + b t)} + \\\\frac{1}{b t (1 + c t)} + \\\\frac{1}{c t (1 + a t)}\\n\\\\]\\n\\nRight side:\\n\\n\\\\[\\n\\\\frac{3}{1 + a\\' b\\' c\\'} = \\\\frac{3}{1 + 1} = \\\\frac{3}{2}\\n\\\\]\\n\\nBut the original inequality scaled would be:\\n\\n\\\\[\\n\\\\frac{1}{a t (1 + b t)} + \\\\frac{1}{b t (1 + c t)} + \\\\frac{1}{c t (1 + a t)} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nBut how does this relate to the original inequality? Let\\'s express the left side in terms of the original variables. Multiply both sides by \\\\(t\\\\):\\n\\nWait, let me see. Let me denote \\\\(t = \\\\frac{1}{(abc)^{1/3}} = (abc)^{-1/3}\\\\). Then \\\\(a t = a \\\\cdot (abc)^{-1/3} = a^{2/3} b^{-1/3} c^{-1/3}\\\\). Similarly for \\\\(b t\\\\) and \\\\(c t\\\\). This seems complicated.\\n\\nAlternatively, note that after scaling, we have:\\n\\nThe left side of the scaled inequality is:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a t (1 + b t)} = \\\\frac{1}{t} \\\\sum \\\\frac{1}{a (1 + b t)}\\n\\\\]\\n\\nBut the original inequality\\'s left side is \\\\(\\\\sum \\\\frac{1}{a(1 + b)}\\\\). So unless \\\\(t = 1\\\\), which only happens when \\\\(abc = 1\\\\), they are different. Therefore, this scaling approach might not directly relate the general case to the case \\\\(abc = 1\\\\). Hmm.\\n\\nAlternatively, maybe use the substitution \\\\(x = \\\\ln a\\\\), \\\\(y = \\\\ln b\\\\), \\\\(z = \\\\ln c\\\\), but I\\'m not sure if logarithmic substitution helps here.\\n\\nWait, another idea: use the substitution \\\\(p = abc\\\\). Then we can express one variable in terms of the others, say \\\\(c = \\\\frac{p}{ab}\\\\). Then the inequality becomes a function of \\\\(a\\\\), \\\\(b\\\\), and \\\\(p\\\\). But this might complicate things.\\n\\nAlternatively, let\\'s consider using the method of Lagrange multipliers to find the minimum of the left-hand side under the constraint \\\\(abc = k\\\\). But this might be too advanced for an initial approach, though it\\'s worth considering.\\n\\nLet me try that. Suppose we fix \\\\(abc = k\\\\), and we want to minimize \\\\(f(a, b, c) = \\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)}\\\\). Using Lagrange multipliers, set the gradient of \\\\(f\\\\) equal to Î» times the gradient of the constraint \\\\(g(a, b, c) = abc - k = 0\\\\).\\n\\nCompute partial derivatives:\\n\\nFor \\\\(a\\\\):\\n\\n\\\\[\\n\\\\frac{\\\\partial f}{\\\\partial a} = -\\\\frac{1}{a^2(1 + b)} - \\\\frac{1}{c(1 + a)^2}\\n\\\\]\\n\\nWait, this seems messy. Maybe symmetry suggests that the minimum occurs when \\\\(a = b = c\\\\). Let\\'s check that.\\n\\nIf \\\\(a = b = c = t\\\\), then \\\\(abc = t^3\\\\), and the inequality becomes:\\n\\nLeft side: \\\\(3 \\\\cdot \\\\frac{1}{t(1 + t)} = \\\\frac{3}{t(1 + t)}\\\\)\\n\\nRight side: \\\\(\\\\frac{3}{1 + t^3}\\\\)\\n\\nSo we need to show that:\\n\\n\\\\[\\n\\\\frac{3}{t(1 + t)} \\\\geq \\\\frac{3}{1 + t^3}\\n\\\\]\\n\\nDivide both sides by 3:\\n\\n\\\\[\\n\\\\frac{1}{t(1 + t)} \\\\geq \\\\frac{1}{1 + t^3}\\n\\\\]\\n\\nWhich simplifies to:\\n\\n\\\\[\\n1 + t^3 \\\\geq t(1 + t)\\n\\\\]\\n\\nMultiply both sides by \\\\(t(1 + t)\\\\) (positive):\\n\\n\\\\[\\n(1 + t^3) \\\\geq t(1 + t)\\n\\\\]\\n\\nSimplify the right side:\\n\\n\\\\[\\n1 + t^3 \\\\geq t + t^2\\n\\\\]\\n\\nRearranged:\\n\\n\\\\[\\nt^3 - t^2 - t + 1 \\\\geq 0\\n\\\\]\\n\\nFactor the left side:\\n\\nLet me try factoring \\\\(t^3 - t^2 - t + 1\\\\). Group terms:\\n\\n\\\\[\\n(t^3 - t^2) - (t - 1) = t^2(t - 1) - 1(t - 1) = (t^2 - 1)(t - 1) = (t - 1)(t + 1)(t - 1) = (t - 1)^2(t + 1)\\n\\\\]\\n\\nTherefore,\\n\\n\\\\[\\n(t - 1)^2(t + 1) \\\\geq 0\\n\\\\]\\n\\nSince \\\\(t > 0\\\\), \\\\(t + 1 > 0\\\\), and \\\\((t - 1)^2 \\\\geq 0\\\\), the entire expression is always non-negative. Equality holds when \\\\((t - 1)^2 = 0\\\\), i.e., \\\\(t = 1\\\\). Therefore, when \\\\(a = b = c = t = 1\\\\), equality holds. This suggests that the inequality holds when variables are equal, but does this account for all cases?\\n\\nHowever, this only shows that when \\\\(a = b = c\\\\), the inequality holds, but we need to show it holds for all positive \\\\(a, b, c\\\\). So even though the symmetric case gives the minimal value, we need to ensure that other configurations don\\'t give a smaller left-hand side.\\n\\nBut this approach via Lagrange multipliers or symmetry is not complete. Let me think differently.\\n\\nGoing back to the original substitution where we set \\\\(abc = 1\\\\) and proved the inequality holds. Then, perhaps, for general \\\\(a, b, c\\\\), we can scale them to make \\\\(abc = 1\\\\) and apply the previous result.\\n\\nLet me try this. Let \\\\(abc = k\\\\), and set \\\\(a = \\\\frac{x}{(k)^{1/3}}\\\\), \\\\(b = \\\\frac{y}{(k)^{1/3}}\\\\), \\\\(c = \\\\frac{z}{(k)^{1/3}}\\\\), so that \\\\(xyz = k \\\\cdot \\\\frac{1}{(k)^{1}} = k^{2/3}\\\\). Wait, not helpful. Alternatively, set \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x} \\\\cdot t\\\\), so that \\\\(abc = t\\\\). Then, adjust \\\\(t\\\\) to make \\\\(abc = 1\\\\). Hmm, not sure.\\n\\nWait, here\\'s a better approach. Let\\'s assume \\\\(abc = k\\\\), and define \\\\(a = \\\\frac{x}{k^{1/3}}\\\\), \\\\(b = \\\\frac{y}{k^{1/3}}\\\\), \\\\(c = \\\\frac{z}{k^{1/3}}\\\\), so that \\\\(abc = \\\\frac{xyz}{k} = k\\\\), so \\\\(xyz = k^2\\\\). But this might not lead us anywhere.\\n\\nAlternatively, let\\'s consider homogenizing the original inequality. The original inequality is:\\n\\n\\\\[\\n\\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nMultiply both sides by \\\\(1 + abc\\\\):\\n\\n\\\\[\\n\\\\left( \\\\sum \\\\frac{1}{a(1 + b)} \\\\right)(1 + abc) \\\\geq 3\\n\\\\]\\n\\nAs we did before, expanding gives:\\n\\n\\\\[\\n\\\\sum \\\\frac{1 + bc}{a(1 + b)} \\\\geq 3\\n\\\\]\\n\\nWhich is the same as:\\n\\n\\\\[\\n\\\\sum \\\\frac{1 + bc}{a(1 + b)} \\\\geq 3\\n\\\\]\\n\\nNow, let\\'s apply the substitution \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), which gives \\\\(abc = 1\\\\), and as previously shown, this reduces the left-hand side to \\\\(\\\\frac{yz}{x^2} + \\\\frac{zx}{y^2} + \\\\frac{xy}{z^2} \\\\geq 3\\\\). But this was under the condition \\\\(abc = 1\\\\). So if we can express the general case in terms of this substitution, perhaps using scaling.\\n\\nWait, another idea. Let me assume that in the general case, after scaling variables, the left-hand side can be expressed in terms of the case when \\\\(abc = 1\\\\). Let\\'s let \\\\(k = abc\\\\), and set \\\\(a = a\\' / k^{1/3}\\\\), \\\\(b = b\\' / k^{1/3}\\\\), \\\\(c = c\\' / k^{1/3}\\\\), so that \\\\(a\\' b\\' c\\' = 1\\\\). Then substituting into the original inequality:\\n\\nLeft side:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{(a\\' / k^{1/3})(1 + b\\' / k^{1/3})} = \\\\sum \\\\frac{k^{1/3}}{a\\'(1 + b\\' / k^{1/3})}\\n\\\\]\\n\\nRight side:\\n\\n\\\\[\\n\\\\frac{3}{1 + (a\\' / k^{1/3})(b\\' / k^{1/3})(c\\' / k^{1/3})} = \\\\frac{3}{1 + (a\\' b\\' c\\') / k} = \\\\frac{3}{1 + 1/k} = \\\\frac{3k}{1 + k}\\n\\\\]\\n\\nTherefore, the inequality becomes:\\n\\n\\\\[\\nk^{1/3} \\\\left( \\\\sum \\\\frac{1}{a\\'(1 + b\\' / k^{1/3})} \\\\right) \\\\geq \\\\frac{3k}{1 + k}\\n\\\\]\\n\\nBut this seems complicated. Maybe not helpful.\\n\\nAlternatively, let\\'s consider the following identity or inequality. Notice that in the case when \\\\(abc = 1\\\\), we have proven the inequality. So perhaps for the general case, if we set \\\\(k = abc\\\\), then we can relate the left-hand side and right-hand side using the case \\\\(abc = 1\\\\).\\n\\nLet me write the original inequality as:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nLet me multiply both sides by \\\\(1 + abc\\\\):\\n\\n\\\\[\\n\\\\sum \\\\frac{1 + abc}{a(1 + b)} \\\\geq 3\\n\\\\]\\n\\nWhich is equivalent to:\\n\\n\\\\[\\n\\\\sum \\\\left( \\\\frac{1}{a(1 + b)} + \\\\frac{bc}{1 + b} \\\\right) \\\\geq 3\\n\\\\]\\n\\nWait, expanding \\\\( \\\\frac{1 + abc}{a(1 + b)} = \\\\frac{1}{a(1 + b)} + \\\\frac{abc}{a(1 + b)} = \\\\frac{1}{a(1 + b)} + \\\\frac{bc}{1 + b} \\\\)\\n\\nSo the inequality becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} + \\\\sum \\\\frac{bc}{1 + b} \\\\geq 3\\n\\\\]\\n\\nBut this separates the original left-hand side into two sums. However, I\\'m not sure if this helps. Let me compute the second sum:\\n\\n\\\\[\\n\\\\sum \\\\frac{bc}{1 + b} = \\\\frac{bc}{1 + b} + \\\\frac{ac}{1 + c} + \\\\frac{ab}{1 + a}\\n\\\\]\\n\\nWait, this seems similar to the original left-hand side but with different variables. Not sure.\\n\\nAlternatively, perhaps use the following substitution: let \\\\(x = bc\\\\), \\\\(y = ac\\\\), \\\\(z = ab\\\\). Then \\\\(abc = \\\\sqrt{x y z}\\\\). But this might not be helpful.\\n\\nAlternatively, notice that \\\\( \\\\frac{bc}{1 + b} = \\\\frac{bc}{1 + b} \\\\). Maybe relate this to the term \\\\(\\\\frac{1}{b(1 + c)}\\\\) from the original left-hand side.\\n\\nAlternatively, consider that the second sum \\\\(\\\\sum \\\\frac{bc}{1 + b}\\\\) is similar to the original left-hand side but cyclically shifted.\\n\\nWait, maybe adding the two sums:\\n\\nOriginal left-hand side \\\\(\\\\sum \\\\frac{1}{a(1 + b)}\\\\) and the second sum \\\\(\\\\sum \\\\frac{bc}{1 + b}\\\\). If I add them together, perhaps there\\'s a way to apply AM-GM or another inequality.\\n\\nBut I\\'m getting stuck here. Let me revisit the earlier approach where we used substitution \\\\(abc = 1\\\\) and proved the inequality. If the inequality holds for \\\\(abc = 1\\\\), perhaps we can use the fact that the inequality is \"homogeneous\" in some transformed variables.\\n\\nWait, another idea: let\\'s make substitution \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), then \\\\(abc = 1\\\\), and the inequality becomes \\\\(\\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{3}{2}\\\\). But in this substitution, it\\'s equivalent to the original inequality when \\\\(abc = 1\\\\). So for the general case, when \\\\(abc = k\\\\), perhaps we can scale variables accordingly.\\n\\nWait, here\\'s a different approach inspired by the substitution method. Let me define \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x} \\\\cdot t\\\\). Then \\\\(abc = \\\\frac{x}{y} \\\\cdot \\\\frac{y}{z} \\\\cdot \\\\frac{z t}{x} = t\\\\). So by setting \\\\(t = abc\\\\), we can adjust \\\\(c\\\\) accordingly. Then substitute into the inequality:\\n\\nLeft side:\\n\\n\\\\[\\n\\\\frac{1}{a(1 + b)} + \\\\frac{1}{b(1 + c)} + \\\\frac{1}{c(1 + a)} = \\\\frac{1}{\\\\frac{x}{y}(1 + \\\\frac{y}{z})} + \\\\frac{1}{\\\\frac{y}{z}(1 + \\\\frac{z t}{x})} + \\\\frac{1}{\\\\frac{z t}{x}(1 + \\\\frac{x}{y})}\\n\\\\]\\n\\nSimplify each term:\\n\\nFirst term:\\n\\n\\\\[\\n\\\\frac{y}{x(1 + \\\\frac{y}{z})} = \\\\frac{y}{x \\\\cdot \\\\frac{z + y}{z}} = \\\\frac{y z}{x(z + y)}\\n\\\\]\\n\\nSecond term:\\n\\n\\\\[\\n\\\\frac{z}{y(1 + \\\\frac{z t}{x})} = \\\\frac{z}{y \\\\cdot \\\\frac{x + z t}{x}} = \\\\frac{z x}{y(x + z t)}\\n\\\\]\\n\\nThird term:\\n\\n\\\\[\\n\\\\frac{x}{z t(1 + \\\\frac{x}{y})} = \\\\frac{x}{z t \\\\cdot \\\\frac{y + x}{y}} = \\\\frac{x y}{z t(y + x)}\\n\\\\]\\n\\nTherefore, the left-hand side becomes:\\n\\n\\\\[\\n\\\\frac{y z}{x(z + y)} + \\\\frac{z x}{y(x + z t)} + \\\\frac{x y}{z t(y + x)}\\n\\\\]\\n\\nThe right-hand side is:\\n\\n\\\\[\\n\\\\frac{3}{1 + abc} = \\\\frac{3}{1 + t}\\n\\\\]\\n\\nSo we need to show:\\n\\n\\\\[\\n\\\\frac{y z}{x(z + y)} + \\\\frac{z x}{y(x + z t)} + \\\\frac{x y}{z t(y + x)} \\\\geq \\\\frac{3}{1 + t}\\n\\\\]\\n\\nThis seems quite complex. Maybe set \\\\(x = y = z\\\\) to check for equality. Let me set \\\\(x = y = z = 1\\\\). Then the left side becomes:\\n\\n\\\\[\\n\\\\frac{1 * 1}{1(1 + 1)} + \\\\frac{1 * 1}{1(1 + t)} + \\\\frac{1 * 1}{1 * t(1 + 1)} = \\\\frac{1}{2} + \\\\frac{1}{1 + t} + \\\\frac{1}{2t}\\n\\\\]\\n\\nThe right side is \\\\(\\\\frac{3}{1 + t}\\\\). Let\\'s check if the inequality holds:\\n\\nIs \\\\(\\\\frac{1}{2} + \\\\frac{1}{1 + t} + \\\\frac{1}{2t} \\\\geq \\\\frac{3}{1 + t}\\\\)?\\n\\nSubtract \\\\(\\\\frac{3}{1 + t}\\\\) from both sides:\\n\\n\\\\[\\n\\\\frac{1}{2} + \\\\frac{1}{1 + t} + \\\\frac{1}{2t} - \\\\frac{3}{1 + t} = \\\\frac{1}{2} - \\\\frac{2}{1 + t} + \\\\frac{1}{2t} \\\\geq 0\\n\\\\]\\n\\nMultiply all terms by \\\\(2t(1 + t)\\\\) to eliminate denominators:\\n\\n\\\\[\\nt(1 + t) - 4t + (1 + t) \\\\geq 0\\n\\\\]\\n\\nExpand:\\n\\n\\\\[\\nt + t^2 - 4t + 1 + t = t^2 - 2t + 1 = (t - 1)^2 \\\\geq 0\\n\\\\]\\n\\nWhich is always true. Equality holds when \\\\(t = 1\\\\), which corresponds to \\\\(abc = t = 1\\\\). So in this case, when \\\\(x = y = z = 1\\\\) and \\\\(t = 1\\\\), we have equality. This suggests that the inequality holds even after substitution, but this is only a specific case. However, this gives me confidence that the approach is valid.\\n\\nGiven the complexity of these substitutions, maybe the original substitution approach when \\\\(abc = 1\\\\) is sufficient, and then the general case can be deduced by scaling. However, I need to formalize this.\\n\\nHere\\'s a possible way: assume that for any positive real numbers \\\\(a, b, c\\\\), we can scale them to \\\\(a\\' = a \\\\cdot t\\\\), \\\\(b\\' = b \\\\cdot t\\\\), \\\\(c\\' = c \\\\cdot t\\\\) such that \\\\(a\\' b\\' c\\' = 1\\\\). As before, \\\\(t = \\\\frac{1}{(abc)^{1/3}}\\\\). Then, substituting into the inequality:\\n\\nLeft side becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a\\' (1 + b\\')} = \\\\sum \\\\frac{1}{(a t)(1 + b t)}\\n\\\\]\\n\\nRight side becomes:\\n\\n\\\\[\\n\\\\frac{3}{1 + a\\' b\\' c\\'} = \\\\frac{3}{1 + 1} = \\\\frac{3}{2}\\n\\\\]\\n\\nBut we need to relate this to the original inequality. The original inequality\\'s right side is \\\\(\\\\frac{3}{1 + abc}\\\\). If we substitute \\\\(a\\', b\\', c\\'\\\\), then \\\\(abc = \\\\frac{1}{t^3}\\\\). So the right side is \\\\(\\\\frac{3}{1 + \\\\frac{1}{t^3}} = \\\\frac{3 t^3}{1 + t^3}\\\\).\\n\\nTherefore, the scaled inequality is:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{(a t)(1 + b t)} \\\\geq \\\\frac{3 t^3}{1 + t^3}\\n\\\\]\\n\\nMultiply both sides by \\\\(t\\\\):\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a (1 + b t)} \\\\geq \\\\frac{3 t^2}{1 + t^3}\\n\\\\]\\n\\nBut the original inequality is:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a (1 + b)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nWhich, with \\\\(abc = \\\\frac{1}{t^3}\\\\), becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a (1 + b)} \\\\geq \\\\frac{3}{1 + \\\\frac{1}{t^3}} = \\\\frac{3 t^3}{1 + t^3}\\n\\\\]\\n\\nComparing with the scaled inequality:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a (1 + b t)} \\\\geq \\\\frac{3 t^2}{1 + t^3}\\n\\\\]\\n\\nBut this doesn\\'t directly relate to the original inequality. It seems that scaling introduces a factor of \\\\(t\\\\) in the denominator term \\\\(1 + b t\\\\), which complicates things.\\n\\nGiven the time I\\'ve spent and the progress made when \\\\(abc = 1\\\\), maybe the best approach is to use the substitution \\\\(abc = k\\\\) and utilize the earlier result. However, I need to find a way to express the original inequality in terms of the case when \\\\(abc = 1\\\\).\\n\\nWait, here\\'s a different strategy inspired by the Cauchy-Schwarz inequality. Let me consider the original inequality:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nLet me apply the Cauchy-Schwarz inequality in the following form:\\n\\n\\\\[\\n\\\\left( \\\\sum \\\\frac{1}{a(1 + b)} \\\\right) \\\\left( \\\\sum a(1 + b) \\\\right) \\\\geq (1 + 1 + 1)^2 = 9\\n\\\\]\\n\\nTherefore,\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{9}{\\\\sum a(1 + b)}\\n\\\\]\\n\\nSo if I can show that \\\\(\\\\sum a(1 + b) \\\\leq 3(1 + abc)\\\\), then it would follow that:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{9}{3(1 + abc)} = \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nWhich is exactly what we need. So the key is to prove that \\\\(\\\\sum a(1 + b) \\\\leq 3(1 + abc)\\\\).\\n\\nCompute \\\\(\\\\sum a(1 + b) = a + b + c + ab + bc + ca\\\\).\\n\\nWe need to show:\\n\\n\\\\[\\na + b + c + ab + bc + ca \\\\leq 3(1 + abc)\\n\\\\]\\n\\nIs this true? Let\\'s check for \\\\(a = b = c = 1\\\\): Left side is \\\\(3 + 3 = 6\\\\), right side is \\\\(3(2) = 6\\\\). Equality holds. What about \\\\(a = b = c = 2\\\\): Left side \\\\(6 + 12 = 18\\\\), right side \\\\(3(1 + 8) = 27\\\\). Indeed, 18 â‰¤ 27. For \\\\(a = b = c = 0.5\\\\): Left side \\\\(1.5 + 0.75 = 2.25\\\\), right side \\\\(3(1 + 0.125) = 3.375\\\\). 2.25 â‰¤ 3.375. So it seems to hold in these cases. But is this inequality true in general?\\n\\nWait, but this is the opposite of what we need. The inequality \\\\(\\\\sum a(1 + b) \\\\leq 3(1 + abc)\\\\) needs to be proven to apply the C-S approach. However, I\\'m not sure if this is generally true. Let\\'s test another case. Let \\\\(a = 2\\\\), \\\\(b = 2\\\\), \\\\(c = 0.25\\\\). Then left side: 2 + 2 + 0.25 + 4 + 0.5 + 0.5 = 9.25. Right side: 3(1 + 1) = 6. Here, 9.25 > 6, which contradicts the inequality. Hence, this approach is invalid.\\n\\nTherefore, the Cauchy-Schwarz approach in this manner doesn\\'t work because the inequality \\\\(\\\\sum a(1 + b) \\\\leq 3(1 + abc)\\\\) is not always true. Hence, this path is incorrect.\\n\\nGoing back to the earlier substitution where we set \\\\(abc = 1\\\\) and proved the inequality, and noting that equality holds at \\\\(a = b = c = 1\\\\), perhaps we can use the method of mixing variables or consider using the AM-GM inequality in a more clever way.\\n\\nAnother idea: Let\\'s use the substitution \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), then \\\\(abc = 1\\\\), and the inequality becomes:\\n\\n\\\\[\\n\\\\frac{yz}{x(y + z)} + \\\\frac{zx}{y(z + x)} + \\\\frac{xy}{z(x + y)} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nThis is similar to Nesbitt\\'s inequality but with products in the numerators. Wait, actually, this is the same as the inequality we derived earlier. And we proved it using Cauchy-Schwarz and AM-GM, showing it\\'s at least 3/2. Thus, when \\\\(abc = 1\\\\), the inequality holds.\\n\\nNow, for the general case, since we\\'ve proven it when \\\\(abc = 1\\\\), perhaps we can use the following scaling argument. Let \\\\(k = abc\\\\), and set \\\\(a\\' = a / k^{1/3}\\\\), \\\\(b\\' = b / k^{1/3}\\\\), \\\\(c\\' = c / k^{1/3}\\\\). Then \\\\(a\\' b\\' c\\' = 1\\\\), and applying the inequality for \\\\(a\\', b\\', c\\'\\\\):\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a\\'(1 + b\\')} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nBut the left side is:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{(a / k^{1/3})(1 + b / k^{1/3})} = k^{1/3} \\\\sum \\\\frac{1}{a(1 + b / k^{1/3})}\\n\\\\]\\n\\nAnd the inequality becomes:\\n\\n\\\\[\\nk^{1/3} \\\\sum \\\\frac{1}{a(1 + b / k^{1/3})} \\\\geq \\\\frac{3}{2}\\n\\\\]\\n\\nBut the original inequality we need to prove is:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{3}{1 + k}\\n\\\\]\\n\\nThis seems different. However, if we set \\\\(t = k^{1/3}\\\\), then \\\\(k = t^3\\\\), and the inequality becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{a(1 + b)} \\\\geq \\\\frac{3}{1 + t^3}\\n\\\\]\\n\\nBut how does this relate to the scaled inequality? Not directly. I\\'m stuck here.\\n\\nPerhaps I need to consider a different inequality or a transformation that can directly relate the general case to the case when \\\\(abc = 1\\\\). Another approach could be using the substitution \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x} \\\\cdot s\\\\), where \\\\(s\\\\) is a scaling factor, but this might not lead anywhere.\\n\\nWait, let me try using HÃ¶lder\\'s inequality. HÃ¶lder\\'s inequality states that for positive real numbers and exponents \\\\(p, q\\\\) such that \\\\(1/p + 1/q = 1\\\\):\\n\\n\\\\[\\n\\\\sum a_i b_i \\\\leq \\\\left( \\\\sum a_i^p \\\\right)^{1/p} \\\\left( \\\\sum b_i^q \\\\right)^{1/q}\\n\\\\]\\n\\nBut I\\'m not sure how to apply it here. Alternatively, using HÃ¶lder\\'s in the form:\\n\\n\\\\[\\n\\\\left( \\\\sum \\\\frac{1}{a(1 + b)} \\\\right) \\\\left( \\\\sum a(1 + b) \\\\right) \\\\geq (1 + 1 + 1)^2 = 9\\n\\\\]\\n\\nBut this is the same as the Cauchy-Schwarz approach we tried earlier, which didn\\'t work because \\\\(\\\\sum a(1 + b)\\\\) is not bounded above by \\\\(3(1 + abc)\\\\).\\n\\nGiven the time I\\'ve spent and the progress in the case \\\\(abc = 1\\\\), perhaps it\\'s best to conclude that the inequality can be proven by substituting \\\\(abc = 1\\\\) and using the known inequality, then extending via homogenization.\\n\\nWait, here\\'s a breakthrough. Let\\'s consider the following substitution: let \\\\(x = \\\\frac{1}{a}\\\\), \\\\(y = \\\\frac{1}{b}\\\\), \\\\(z = \\\\frac{1}{c}\\\\). Then \\\\(a = \\\\frac{1}{x}\\\\), \\\\(b = \\\\frac{1}{y}\\\\), \\\\(c = \\\\frac{1}{z}\\\\). The left side becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{1}{\\\\frac{1}{x}(1 + \\\\frac{1}{y})} = \\\\sum \\\\frac{x}{1 + \\\\frac{1}{y}} = \\\\sum \\\\frac{x y}{y + 1}\\n\\\\]\\n\\nThe right side becomes:\\n\\n\\\\[\\n\\\\frac{3}{1 + \\\\frac{1}{x y z}} = \\\\frac{3 x y z}{x y z + 1}\\n\\\\]\\n\\nSo the inequality transforms to:\\n\\n\\\\[\\n\\\\frac{x y}{y + 1} + \\\\frac{y z}{z + 1} + \\\\frac{z x}{x + 1} \\\\geq \\\\frac{3 x y z}{x y z + 1}\\n\\\\]\\n\\nThis is a symmetrical inequality in \\\\(x, y, z\\\\). Now, set \\\\(x y z = k\\\\). If we can prove this inequality for all positive \\\\(x, y, z\\\\), then we have the original inequality.\\n\\nLet me consider the case when \\\\(x = y = z = t\\\\). Then the left side becomes \\\\(3 \\\\cdot \\\\frac{t^2}{t + 1}\\\\) and the right side becomes \\\\(\\\\frac{3 t^3}{t^3 + 1}\\\\). The inequality is:\\n\\n\\\\[\\n\\\\frac{3 t^2}{t + 1} \\\\geq \\\\frac{3 t^3}{t^3 + 1}\\n\\\\]\\n\\nCancel 3 and multiply both sides by \\\\((t + 1)(t^3 + 1)\\\\):\\n\\n\\\\[\\nt^2(t^3 + 1) \\\\geq t^3(t + 1)\\n\\\\]\\n\\nSimplify:\\n\\n\\\\[\\nt^5 + t^2 \\\\geq t^4 + t^3\\n\\\\]\\n\\nRearrange:\\n\\n\\\\[\\nt^5 - t^4 - t^3 + t^2 \\\\geq 0\\n\\\\]\\n\\nFactor:\\n\\n\\\\[\\nt^2(t^3 - t^2 - t + 1) = t^2(t^2(t - 1) - 1(t - 1)) = t^2(t^2 - 1)(t - 1) = t^2(t - 1)^2(t + 1)\\n\\\\]\\n\\nSince \\\\(t > 0\\\\), this expression is always non-negative, with equality when \\\\(t = 1\\\\). This again confirms the equality case.\\n\\nBut this is still specific to \\\\(x = y = z\\\\). To generalize, let\\'s use the substitution \\\\(x y z = k\\\\). Assume \\\\(k\\\\) is fixed, and we need to minimize the left-hand side.\\n\\nUsing AM-GM on the left-hand side terms:\\n\\n\\\\[\\n\\\\frac{x y}{y + 1} + \\\\frac{y z}{z + 1} + \\\\frac{z x}{x + 1} \\\\geq 3 \\\\sqrt[3]{\\\\frac{x y}{y + 1} \\\\cdot \\\\frac{y z}{z + 1} \\\\cdot \\\\frac{z x}{x + 1}} = 3 \\\\sqrt[3]{\\\\frac{x^2 y^2 z^2}{(x + 1)(y + 1)(z + 1)}}\\n\\\\]\\n\\nBut the right-hand side of the original inequality is \\\\(\\\\frac{3 k}{k + 1}\\\\). So we need:\\n\\n\\\\[\\n3 \\\\sqrt[3]{\\\\frac{k^2}{(x + 1)(y + 1)(z + 1)}} \\\\geq \\\\frac{3 k}{k + 1}\\n\\\\]\\n\\nCancel 3:\\n\\n\\\\[\\n\\\\sqrt[3]{\\\\frac{k^2}{(x + 1)(y + 1)(z + 1)}} \\\\geq \\\\frac{k}{k + 1}\\n\\\\]\\n\\nCube both sides:\\n\\n\\\\[\\n\\\\frac{k^2}{(x + 1)(y + 1)(z + 1)} \\\\geq \\\\frac{k^3}{(k + 1)^3}\\n\\\\]\\n\\nSimplify:\\n\\n\\\\[\\n\\\\frac{1}{(x + 1)(y + 1)(z + 1)} \\\\geq \\\\frac{k}{(k + 1)^3}\\n\\\\]\\n\\nWhich implies:\\n\\n\\\\[\\n(x + 1)(y + 1)(z + 1) \\\\leq \\\\frac{(k + 1)^3}{k}\\n\\\\]\\n\\nBut expanding the left side:\\n\\n\\\\[\\n(x + 1)(y + 1)(z + 1) = x y z + x y + y z + z x + x + y + z + 1 = k + x y + y z + z x + x + y + z + 1\\n\\\\]\\n\\nWe need to show that:\\n\\n\\\\[\\nk + x y + y z + z x + x + y + z + 1 \\\\leq \\\\frac{(k + 1)^3}{k}\\n\\\\]\\n\\nThis seems complicated, and I\\'m not sure if this path is correct. Perhaps another approach is needed.\\n\\nWait, going back to the transformed inequality:\\n\\n\\\\[\\n\\\\frac{x y}{y + 1} + \\\\frac{y z}{z + 1} + \\\\frac{z x}{x + 1} \\\\geq \\\\frac{3 x y z}{x y z + 1}\\n\\\\]\\n\\nLet me denote \\\\(S = x y + y z + z x\\\\) and \\\\(P = x y z\\\\). The inequality becomes:\\n\\n\\\\[\\n\\\\sum \\\\frac{x y}{y + 1} \\\\geq \\\\frac{3 P}{P + 1}\\n\\\\]\\n\\nPerhaps relate this to the variables \\\\(S\\\\) and \\\\(P\\\\). But it\\'s not clear.\\n\\nAlternatively, consider the function \\\\(f(t) = \\\\frac{t}{1 + t}\\\\), which is concave for \\\\(t > 0\\\\). Then, by Jensen\\'s inequality:\\n\\n\\\\[\\n\\\\sum \\\\frac{x y}{y + 1} = \\\\sum \\\\frac{x y}{y + 1} = \\\\sum x \\\\cdot \\\\frac{y}{y + 1}\\n\\\\]\\n\\nBut this is a sum of \\\\(x \\\\cdot f(y)\\\\), and since \\\\(f\\\\) is concave, maybe apply Jensen. However, the weights here are \\\\(x, y, z\\\\), which are variables, so not sure.\\n\\nAlternatively, use the substitution \\\\(u = y\\\\), \\\\(v = z\\\\), \\\\(w = x\\\\), but this might not help.\\n\\nGiven the time I\\'ve invested and the multiple approaches tried, I think the key steps are:\\n\\n1. When \\\\(abc = 1\\\\), the inequality can be transformed via substitution to a known form, which can be proven using AM-GM and Cauchy-Schwarz.\\n2. The equality case is when \\\\(a = b = c = 1\\\\).\\n3. The inequality holds due to the symmetry and the application of classic inequalities when \\\\(abc = 1\\\\), and by scaling, it can be extended to the general case.\\n\\nHowever, the exact step-by-step proof for the general case hasn\\'t been fully articulated yet. Given the time constraints, I\\'ll consolidate the valid steps:\\n\\n**Final Answer**\\n\\\\boxed{\\\\frac{1}{a(1+b)}+\\\\frac{1}{b(1+c)}+\\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc}}\\n\\n**Step-by-Step Explanation and Proof:**\\n\\n1. **Substitution for \\\\(abc = 1\\\\):**\\n   - Assume \\\\(abc = 1\\\\). Let \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\), transforming the inequality into:\\n     \\\\[\\n     \\\\frac{yz}{x(y + z)} + \\\\frac{zx}{y(z + x)} + \\\\frac{xy}{z(x + y)} \\\\geq \\\\frac{3}{2}\\n     \\\\]\\n   - Apply the Cauchy-Schwarz inequality:\\n     \\\\[\\n     \\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{(xy + yz + zx)^2}{xyz(y + z) + xyz(z + x) + xyz(x + y)} = \\\\frac{(xy + yz + zx)^2}{2xyz(x + y + z)}\\n     \\\\]\\n   - Simplify using AM-GM to show the sum is at least \\\\(\\\\frac{3}{2}\\\\).\\n\\n2. **AM-GM Application:**\\n   - Use AM-GM on each term \\\\(\\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{\\\\sqrt{yz}}{2x}\\\\).\\n   - Summing up gives:\\n     \\\\[\\n     \\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{1}{2} \\\\left( \\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\right) \\\\geq \\\\frac{3}{2}\\n     \\\\]\\n   - This holds by AM-GM on the terms inside the parentheses.\\n\\n3. **General Case via Scaling:**\\n   - For any \\\\(a, b, c > 0\\\\), let \\\\(k = abc\\\\). Scale variables to \\\\(a\\' = a k^{-1/3}\\\\), \\\\(b\\' = b k^{-1/3}\\\\), \\\\(c\\' = c k^{-1/3}\\\\), so \\\\(a\\' b\\' c\\' = 1\\\\).\\n   - Apply the proven inequality for \\\\(abc = 1\\\\):\\n     \\\\[\\n     \\\\sum \\\\frac{1}{a\\'(1 + b\\')} \\\\geq \\\\frac{3}{2}\\n     \\\\]\\n   - Reverting the scaling confirms the original inequality holds, with equality when \\\\(a = b = c = 1\\\\).\\n\\n4. **Equality Condition:**\\n   - Equality in AM-GM occurs when all terms are equal, leading to \\\\(a = b = c = 1\\\\).\\n\\nThus, the inequality is proven, and equality holds exclusively at \\\\(a = b = c = 1\\\\).\\n\\n<|end_of_thought|>\\n\\n<|begin_of_solution|>\\n\\nTo prove the inequality \\n\\n\\\\[\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{b(1+c)} + \\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc}\\n\\\\]\\n\\nfor positive real numbers \\\\(a, b, c\\\\) with equality if and only if \\\\(a = b = c = 1\\\\), we proceed as follows:\\n\\n### Key Steps and Proof:\\n\\n1. **Substitution for \\\\(abc = 1\\\\):**\\n   - Assume \\\\(abc = 1\\\\). Let \\\\(a = \\\\frac{x}{y}\\\\), \\\\(b = \\\\frac{y}{z}\\\\), \\\\(c = \\\\frac{z}{x}\\\\). This transforms the left-hand side of the inequality into:\\n     \\\\[\\n     \\\\frac{yz}{x(y + z)} + \\\\frac{zx}{y(z + x)} + \\\\frac{xy}{z(x + y)}\\n     \\\\]\\n   - Using the AM-GM inequality on each term, we show:\\n     \\\\[\\n     \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{\\\\sqrt{yz}}{2x}\\n     \\\\]\\n   - Summing up these inequalities and applying AM-GM again, we get:\\n     \\\\[\\n     \\\\sum \\\\frac{yz}{x(y + z)} \\\\geq \\\\frac{1}{2} \\\\left( \\\\frac{\\\\sqrt{yz}}{x} + \\\\frac{\\\\sqrt{zx}}{y} + \\\\frac{\\\\sqrt{xy}}{z} \\\\right) \\\\geq \\\\frac{3}{2}\\n     \\\\]\\n\\n2. **General Case via Scaling:**\\n   - For any positive real numbers \\\\(a, b, c\\\\), let \\\\(k = abc\\\\). Scale the variables to \\\\(a\\' = a k^{-1/3}\\\\), \\\\(b\\' = b k^{-1/3}\\\\), \\\\(c\\' = c k^{-1/3}\\\\) such that \\\\(a\\' b\\' c\\' = 1\\\\).\\n   - Applying the proven inequality for \\\\(abc = 1\\\\), we have:\\n     \\\\[\\n     \\\\sum \\\\frac{1}{a\\'(1 + b\\')} \\\\geq \\\\frac{3}{2}\\n     \\\\]\\n   - Reverting the scaling confirms the original inequality holds for general \\\\(a, b, c\\\\).\\n\\n3. **Equality Condition:**\\n   - Equality in the AM-GM steps occurs when all terms are equal, leading to \\\\(a = b = c = 1\\\\).\\n\\n### Final Answer\\n\\n\\\\[\\n\\\\boxed{\\\\frac{1}{a(1+b)}+\\\\frac{1}{b(1+c)}+\\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc}}\\n\\\\]\\n\\nEquality holds if and only if \\\\(a = b = c = 1\\\\).\\n\\n<|end_of_solution|><|im_end|>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeli eÄŸitmeden Ã¶nce bÃ¼tÃ¼n modeli eÄŸitmek fazlaca yer ve zaman iÃ§ereceÄŸinden dolayÄ± PEFT (Parameter Efficient Fine Tuning) tekniÄŸini kullanÄ±rÄ±z. Bu bize belli katmanlar Ã¼zerinde model eÄŸitimini saÄŸlar ve bÃ¼tÃ¼n modeli eÄŸitmektense modelin baÅŸlÄ± kÄ±sÄ±mlarÄ±nÄ± eÄŸitiriz. AyrÄ±ca PEFT ile gÃ¼nÃ¼mÃ¼zde sÄ±kÃ§a kullanÄ±lan ve PEFT'in alt yÃ¶ntemlerinden biri olan LORA (Low Rank Adaptation) tekniÄŸini de sÄ±kÃ§a kullanÄ±rÄ±z.\n",
    "\n",
    "### PEFTâ€™in alt yÃ¶ntemleri:\n",
    "\n",
    " PEFT bir Ã§atÄ± kavramdÄ±r. AltÄ±nda farklÄ± teknikler vardÄ±r:\n",
    "\n",
    "###\n",
    "Teknik\t        AÃ§Ä±klama\n",
    "\n",
    "LoRA:\t        En popÃ¼ler yÃ¶ntem. AÄŸÄ±rlÄ±klara dÃ¼ÅŸÃ¼k-rank matris ekler.\n",
    "\n",
    "Prefix Tuning:\tGirdi dizisine â€œÃ¶zel tokenlarâ€ ekleyerek ayar yapar.\n",
    "\n",
    "Adapter Tuning:\tHer katmana kÃ¼Ã§Ã¼k ek sinir aÄŸÄ± modÃ¼lleri ekler.\n",
    "\n",
    "Prompt Tuning:\tModeli yeniden eÄŸitmeden, giriÅŸ metnine sanal promptlar ekler.\n",
    "###\n",
    "\n",
    "BugÃ¼n en Ã§ok kullanÄ±lanÄ± LoRAâ€™dÄ±r.\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA, bÃ¼yÃ¼k matrisleri tamamen gÃ¼ncellemek yerine, onlara kÃ¼Ã§Ã¼k, dÃ¼ÅŸÃ¼k-rank eklemeler yapar. Bir model katmanÄ±nda normalde ÅŸÃ¶yle bir aÄŸÄ±rlÄ±k vardÄ±r:\n",
    "\n",
    "ğ‘¦ = ğ‘Š * ğ‘¥\n",
    "\n",
    "Burada ğ‘Š genelde Ã§ok bÃ¼yÃ¼k (Ã¶rneÄŸin 4096Ã—4096 boyutunda) bir matristir. LoRA bunu ÅŸu ÅŸekilde deÄŸiÅŸtirir:\n",
    "\n",
    "ğ‘¦ =(ğ‘Š+ğ´ğµ)ğ‘¥\n",
    "\n",
    "Burada ğ´ ve ğµ kÃ¼Ã§Ã¼k, dÃ¼ÅŸÃ¼k-rank matrislerdir. ğ‘Ÿ bu matrislerin boyutunu belirleyen parametre (Ã¶rneÄŸin 4, 8, 16). EÄŸitim sÄ±rasÄ±nda ğ‘Š sabit kalÄ±r (freeze). Sadece ğ´ ve ğµ Ã¶ÄŸrenilir. Bu sayede parametre sayÄ±sÄ± 100 kat azalabilir. Performans Ã§oÄŸu zaman aynÄ± kalÄ±r veya Ã§ok az dÃ¼ÅŸer. Bu sayede eÄŸitilecek parametre sayÄ±sÄ± azalÄ±r, bellekte tasarruf saÄŸlanÄ±r ve eÄŸitim hÄ±zlanÄ±r.\n",
    "\n",
    "![image.png](attachment:c07ed10e-db72-4371-9bef-f25967c697c6.png)\n",
    "\n",
    "### PEFT + LoRA birlikte nasÄ±l Ã§alÄ±ÅŸÄ±r?\n",
    "\n",
    "PEFT, â€œhangi parametreleri eÄŸitelim?â€ sorusuna genel cevaptÄ±r. LoRA, bu stratejilerden biridir (en verimlisi). PEFT Ã¶nceden eÄŸitilmiÅŸ katmanlarÄ± alÄ±r ve belirli modÃ¼llere LoRA ekler. LoRA ise bu katmanlarÄ± eÄŸitir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:21:06.148188Z",
     "iopub.status.busy": "2025-10-23T07:21:06.147839Z",
     "iopub.status.idle": "2025-10-23T07:22:33.072831Z",
     "shell.execute_reply": "2025-10-23T07:22:33.071985Z",
     "shell.execute_reply.started": "2025-10-23T07:21:06.148159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca372f82f157436e92bf685bcc85533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds,\n",
    "    eval_dataset = None, \n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # 4 adÄ±mda gradyanlar toplansÄ±n\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 3, # Full training iÃ§in 1 ata\n",
    "        #Modeli test ettiÄŸimiz iÃ§in max adÄ±m sayÄ±sÄ± belirliyoruz.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Uzun traininglerde 2e-5 e dÃ¼ÅŸÃ¼r\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Wandb, tensorboard vs kullanmak istemiyorsak none\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:22:33.074571Z",
     "iopub.status.busy": "2025-10-23T07:22:33.073969Z",
     "iopub.status.idle": "2025-10-23T07:22:52.416800Z",
     "shell.execute_reply": "2025-10-23T07:22:52.416058Z",
     "shell.execute_reply.started": "2025-10-23T07:22:33.074544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb5cf4e3bcc49ad943466111a60e6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T07:22:52.419985Z",
     "iopub.status.busy": "2025-10-23T07:22:52.419749Z",
     "iopub.status.idle": "2025-10-23T08:29:46.205292Z",
     "shell.execute_reply": "2025-10-23T08:29:46.204701Z",
     "shell.execute_reply.started": "2025-10-23T07:22:52.419964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:05:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.389800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:29:46.206356Z",
     "iopub.status.busy": "2025-10-23T08:29:46.206082Z",
     "iopub.status.idle": "2025-10-23T08:31:06.597633Z",
     "shell.execute_reply": "2025-10-23T08:31:06.596861Z",
     "shell.execute_reply.started": "2025-10-23T08:29:46.206338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, so I need to find the value of f(4) for the function f(x) = 3xÂ² - 5x + 2. Let me think about how to approach this. \n",
      "\n",
      "First, I remember that when you have a function like this, substituting the value of x into the equation will give you the result. So, f(4) means I should replace every x in the function with 4 and then perform the arithmetic operations. Let me write that down step by step to make sure I don't make a mistake.\n",
      "\n",
      "Starting with the function: f(x) = 3xÂ² - 5x + 2. \n",
      "\n",
      "Substituting x = 4 into the function:\n",
      "\n",
      "f(4) = 3*(4)Â² - 5*(4) + 2.\n",
      "\n",
      "Now, I need to calculate each term separately. Let's start with the first term, 3*(4)Â². \n",
      "\n",
      "First, calculate the exponent: 4 squared is 16. Then multiply by 3: 3*16 = 48. So the first term is 48.\n",
      "\n",
      "Next term is -5*(4). Multiplying -5 by 4 gives me -20.\n",
      "\n",
      "The last term is just +2, so that's straightforward.\n",
      "\n",
      "Now, adding all the terms together: 48 (from the first term) minus 20 (from the second term) plus 2 (from the third term). \n",
      "\n",
      "Let me do the arithmetic step by step. \n",
      "\n",
      "First, 48 - 20. That equals 28. Then, 28 + 2 equals 30. \n",
      "\n",
      "So, putting it all together, f(4) should be 30. \n",
      "\n",
      "Wait, let me double-check to make sure I didn't make any mistakes. Let me go through the calculation again.\n",
      "\n",
      "Starting with f(4) = 3*(4)^2 -5*(4) +2.\n",
      "\n",
      "Calculating each part:\n",
      "\n",
      "3*(4)^2: 4 squared is 16, times 3 is 48.\n",
      "\n",
      "-5*(4): that's -20.\n",
      "\n",
      "Adding the third term: +2.\n",
      "\n",
      "So 48 -20 is 28, then 28 +2 is 30. \n",
      "\n",
      "Hmm, seems consistent. Maybe I can plug it back in to check. Let me see. If x is 4, then 3xÂ² is 3*16=48, -5x is -20, and +2. So 48-20=28, 28+2=30. Yep, that's correct. \n",
      "\n",
      "Alternatively, maybe I can graph the function mentally. The quadratic function opens upwards because the coefficient of xÂ² is positive. At x=4, the value is 30, which seems reasonable. If I think about the vertex of the parabola, the vertex is at x = -b/(2a) = 5/(2*3) = 5/6 â‰ˆ 0.833. So the vertex is at xâ‰ˆ0.833, and since the parabola opens upwards, the minimum value is at the vertex. Then, as x increases beyond 0.833, the function increases. So at x=4, which is to the right of the vertex, the function value should be higher than the vertex. Let's see, the vertex's y-coordinate can be calculated by plugging x=5/6 into the function, but maybe that's overcomplicating. However, since 30 is greater than the vertex value (which would be lower), it makes sense that f(4)=30 is correct.\n",
      "\n",
      "Alternatively, maybe I can compute the value using another method. Let's see, expanding the function might not help here, but perhaps factoring? Let me check if the quadratic can be factored. The function is 3xÂ² -5x +2. To factor it, we look for two numbers that multiply to 3*2=6 and add to -5. The numbers would be -2 and -3. So, 3xÂ² -5x +2 = (3x -2)(x -1). Let me verify: (3x -2)(x -1) = 3xÂ² -3x -2x +2 = 3xÂ² -5x +2. Correct. \n",
      "\n",
      "So, f(x) = (3x -2)(x -1). Then, f(4) would be (3*4 -2)(4 -1) = (12 -2)(3) = 10*3 = 30. Yep, same result.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"If f(x)=3x2âˆ’5x+2, find the value of f(4).\"}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1000, \n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, \n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:31:06.598776Z",
     "iopub.status.busy": "2025-10-23T08:31:06.598538Z",
     "iopub.status.idle": "2025-10-23T08:31:07.717876Z",
     "shell.execute_reply": "2025-10-23T08:31:07.717237Z",
     "shell.execute_reply.started": "2025-10-23T08:31:06.598759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:37:13.301746Z",
     "iopub.status.busy": "2025-10-23T08:37:13.301427Z",
     "iopub.status.idle": "2025-10-23T08:37:28.354220Z",
     "shell.execute_reply": "2025-10-23T08:37:28.353209Z",
     "shell.execute_reply.started": "2025-10-23T08:37:13.301719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/lora_model/ (stored 0%)\n",
      "  adding: kaggle/working/lora_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/lora_model/adapter_config.json (deflated 56%)\n",
      "  adding: kaggle/working/lora_model/merges.txt (deflated 57%)\n",
      "  adding: kaggle/working/lora_model/chat_template.jinja (deflated 76%)\n",
      "  adding: kaggle/working/lora_model/tokenizer_config.json (deflated 90%)\n",
      "  adding: kaggle/working/lora_model/README.md (deflated 65%)\n",
      "  adding: kaggle/working/lora_model/added_tokens.json (deflated 68%)\n",
      "  adding: kaggle/working/lora_model/vocab.json (deflated 61%)\n",
      "  adding: kaggle/working/lora_model/special_tokens_map.json (deflated 69%)\n",
      "  adding: kaggle/working/lora_model/tokenizer.json (deflated 81%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r lora_model.zip /kaggle/working/lora_model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
