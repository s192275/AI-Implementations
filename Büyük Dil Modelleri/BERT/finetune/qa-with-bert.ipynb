{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13215638,"sourceType":"datasetVersion","datasetId":8376465}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport json\nfrom datasets import Dataset, load_dataset\n\nmodel_name = \"dbmdz/bert-base-turkish-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name) \nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:13:41.847979Z","iopub.execute_input":"2025-09-30T10:13:41.848262Z","iopub.status.idle":"2025-09-30T10:14:21.886998Z","shell.execute_reply.started":"2025-09-30T10:13:41.848198Z","shell.execute_reply":"2025-09-30T10:14:21.886360Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f925212fceb412cbc64e0b47018c8ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b7af7b18c94dbfa90bbe1f03f3e72a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27264df764264efe825440e6c8074dac"}},"metadata":{}},{"name":"stderr","text":"2025-09-30 10:14:04.078958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759227244.417365      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759227244.506833      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3078af8a3b1646ae99f3005122db7147"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"max_length = 384 \ndoc_stride = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:21.888616Z","iopub.execute_input":"2025-09-30T10:14:21.889094Z","iopub.status.idle":"2025-09-30T10:14:21.892548Z","shell.execute_reply.started":"2025-09-30T10:14:21.889075Z","shell.execute_reply":"2025-09-30T10:14:21.892015Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"with open(\"/kaggle/input/bert-dataset-qa/train-v0.1.json\", \"r\") as f:\n    data = json.load(f)\n\nrecords = []\nfor entry in data[\"data\"]:\n    for paragraph in entry[\"paragraphs\"]:\n        context = paragraph[\"context\"]\n        for qa in paragraph[\"qas\"]:\n            question = qa[\"question\"]\n            qid = qa[\"id\"]\n            answers = qa[\"answers\"] \n            for ans in answers:\n                records.append({\n                    \"id\": qid,\n                    \"question\": question,\n                    \"context\": context,\n                    \"answer_text\": ans[\"text\"],\n                    \"answer_start\": int(ans[\"answer_start\"])\n                })\n\ndataset = Dataset.from_list(records)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:21.893223Z","iopub.execute_input":"2025-09-30T10:14:21.893494Z","iopub.status.idle":"2025-09-30T10:14:22.157038Z","shell.execute_reply.started":"2025-09-30T10:14:21.893468Z","shell.execute_reply":"2025-09-30T10:14:22.156269Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:22.157873Z","iopub.execute_input":"2025-09-30T10:14:22.158177Z","iopub.status.idle":"2025-09-30T10:14:22.168865Z","shell.execute_reply.started":"2025-09-30T10:14:22.158151Z","shell.execute_reply":"2025-09-30T10:14:22.168175Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'id': 1,\n 'question': 'el-Hasan b. Muhammed el-Vezzan isimli bilgin avrupa’da nasıl tanınmaktadır ?',\n 'context': 'İslam dünyasında bilimin 16. yüzyılda hala yüksek seviyede bulunduğunu gösteren çok ilginç bir örneği deskriptif coğrafya ekolünden verebiliriz. Bize bu örneği, Avrupa’da Afrikalı Leo (Leo Africanus) olarak tanınan el-hasan b. Muhammed el-Vezzan (doğumu yaklaşık 888/1483)’dır. Fas (Fez) şehrinde büyümüş ve eğitimini almış olan Granada doğumlu bu bilgin, diplomatik hizmetler yoluyla, özellikle kuzey Afrika’da olmak üzere birçok İslam ülkesini tanıyıp bir yazar olarak coğrafya ve kartografya ile ilgileniyordu. İstanbul’dan dönüş yolculuğunda Sicilyalı korsanların eline esir düşmüş, ilk olarak Napoli’ye daha sonra Roma’ya satılıp Papa X. Leo tarafından 6.1.1520 yılında bizzat Papa’nın adıyla Giovanni Leo olarak vaftiz edilmişti. İtalya’daki ikameti sırasında İtalyanca öğrendi ve Arapça öğretti. Yazar olarak faaliyetlerini Roma ve Bologna’da devam ettirdi. Afrika coğrafyası dışında kuzey Afrikalı 30 bilginin biyografilerini içeren diğer bir eser derledi. Afrika kitabını esaretinin 6. yılı olan 1526’da İtalyan dilinde tamamladı. 935/1529 yılında Tunus’a döndü ve orada Müslüman olarak öldü.',\n 'answer_text': 'Afrikalı Leo',\n 'answer_start': 171}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def prepare_train_features(examples):\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=384,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answer = examples[\"answer_text\"][sample_index]\n        start_char = int(examples[\"answer_start\"][sample_index])\n        end_char = start_char + len(answer)\n\n        # context kısmının token sınırlarını bul\n        token_start_index = 0\n        while sequence_ids[token_start_index] != 1:\n            token_start_index += 1\n\n        token_end_index = len(input_ids) - 1\n        while sequence_ids[token_end_index] != 1:\n            token_end_index -= 1\n\n        # Eğer cevap context dışında kalıyorsa → CLS’ye yönlendir\n        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            # cevap span’ini token’lara hizala\n            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                token_start_index += 1\n            start_positions.append(token_start_index - 1)\n\n            while offsets[token_end_index][1] >= end_char:\n                token_end_index -= 1\n            end_positions.append(token_end_index + 1)\n\n    tokenized_examples[\"start_positions\"] = start_positions\n    tokenized_examples[\"end_positions\"] = end_positions\n\n    return tokenized_examples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:22.169641Z","iopub.execute_input":"2025-09-30T10:14:22.169871Z","iopub.status.idle":"2025-09-30T10:14:22.184849Z","shell.execute_reply.started":"2025-09-30T10:14:22.169855Z","shell.execute_reply":"2025-09-30T10:14:22.184326Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenized_dataset = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:22.185680Z","iopub.execute_input":"2025-09-30T10:14:22.185926Z","iopub.status.idle":"2025-09-30T10:14:27.625949Z","shell.execute_reply.started":"2025-09-30T10:14:22.185900Z","shell.execute_reply":"2025-09-30T10:14:27.625343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732876ffc3154bf8bce28318f3336019"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-qa\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=100,          \n    disable_tqdm = False,\n    report_to=\"none\"           # wandb vb. loglama servislerini kapat\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset.select(range(100)),  # küçük bir parça eval için\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:14:27.627350Z","iopub.execute_input":"2025-09-30T10:14:27.628097Z","iopub.status.idle":"2025-09-30T10:37:56.635823Z","shell.execute_reply.started":"2025-09-30T10:14:27.628076Z","shell.execute_reply":"2025-09-30T10:37:56.635094Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1574120214.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1020/1020 23:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.279500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.798300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.568000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.308200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.201900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.175300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.073500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.952800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.925700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.961300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1020, training_loss=1.4142309394537234, metrics={'train_runtime': 1405.6744, 'train_samples_per_second': 23.216, 'train_steps_per_second': 0.726, 'total_flos': 6395368769491968.0, 'train_loss': 1.4142309394537234, 'epoch': 3.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import pipeline\n\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\nresult = qa_pipeline({\n    \"context\": \"Türkiye'nin başkenti tarihsel olarak bir dönem İstanbul olsa da şu anki başkenti Ankara'dır.\",\n    \"question\": \"Türkiye'nin başkenti neresidir?\"\n})\n\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:38:20.877370Z","iopub.execute_input":"2025-09-30T10:38:20.877685Z","iopub.status.idle":"2025-09-30T10:38:21.286040Z","shell.execute_reply.started":"2025-09-30T10:38:20.877662Z","shell.execute_reply":"2025-09-30T10:38:21.285191Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"{'score': 0.20872120559215546, 'start': 81, 'end': 87, 'answer': 'Ankara'}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\ntext = r\"\"\" \nAnkara'nın başkent ilan edilmesinin ardından (13 Ekim 1923) şehir hızla gelişmiş ve Türkiye'nin ikinci en kalabalık ili olmuştur. \nTürkiye Cumhuriyeti'nin ilk yıllarında ekonomisi tarım ve hayvancılığa dayanan ilin topraklarının yarısı hâlâ tarım amaçlı kullanılmaktadır. \nEkonomik etkinlik büyük oranda ticaret ve sanayiye dayalıdır. Tarım ve hayvancılığın ağırlığı ise giderek azalmaktadır. \nAnkara ve civarındaki gerek kamu sektörü gerek özel sektör yatırımları, başka illerden büyük bir nüfus göçünü teşvik etmiştir. \nCumhuriyetin kuruluşundan günümüze, nüfusu ülke nüfusunun iki katı hızda artmıştır. \nNüfusun yaklaşık dörtte üçü hizmet sektörü olarak tanımlanabilecek memuriyet, ulaşım, haberleşme ve ticaret benzeri işlerde, dörtte biri sanayide, \n%2'si ise tarım alanında çalışır. Sanayi, özellikle tekstil, gıda ve inşaat sektörlerinde yoğunlaşmıştır. Günümüzde ise en çok savunma, metal ve motor sektörlerinde \nyatırım yapılmaktadır. Türkiye'nin en çok sayıda üniversiteye sahip ili olan Ankara'da ayrıca, üniversite diplomalı kişi oranı ülke ortalamasının iki katıdır.\nBu eğitimli nüfus, teknoloji ağırlıklı yatırımların gereksinim duyduğu iş gücünü oluşturur. \nAnkara'dan otoyollar, demir yolu ve hava yoluyla Türkiye'nin diğer şehirlerine ulaşılır. \nAnkara aynı zamanda başkent olarak Türkiye Büyük Millet Meclisi (TBMM)'ye de ev sahipliği yapmaktadır. \n\"\"\"\n\nquestions = [\n    \"Ankara kaç yılında başkent oldu?\",\n    \"Ankara ne zaman başkent oldu?\",\n    \"Ankara'dan başka şehirlere nasıl ulaşılır?\",\n    \"TBMM neyin kısaltmasıdır?\"\n]\n\nfor question in questions:\n    inputs = tokenizer(question, text, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to('cuda') for k, v in inputs.items()}  # GPU'ya taşı\n\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    input_ids = inputs[\"input_ids\"][0]\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end].cpu())\n    )\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:47:28.661470Z","iopub.execute_input":"2025-09-30T10:47:28.662175Z","iopub.status.idle":"2025-09-30T10:47:28.777164Z","shell.execute_reply.started":"2025-09-30T10:47:28.662149Z","shell.execute_reply":"2025-09-30T10:47:28.776425Z"}},"outputs":[{"name":"stdout","text":"Question: Ankara kaç yılında başkent oldu?\nAnswer: 13 Ekim 1923\n\nQuestion: Ankara ne zaman başkent oldu?\nAnswer: 13 Ekim 1923 )\n\nQuestion: Ankara'dan başka şehirlere nasıl ulaşılır?\nAnswer: otoyollar, demir yolu ve hava yoluyla\n\nQuestion: TBMM neyin kısaltmasıdır?\nAnswer: \n\n","output_type":"stream"}],"execution_count":18}]}