{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:17:54.548749Z","iopub.execute_input":"2025-09-29T13:17:54.548974Z","iopub.status.idle":"2025-09-29T13:17:57.830961Z","shell.execute_reply.started":"2025-09-29T13:17:54.548951Z","shell.execute_reply":"2025-09-29T13:17:57.830257Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport datasets\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:17:57.832038Z","iopub.execute_input":"2025-09-29T13:17:57.832753Z","iopub.status.idle":"2025-09-29T13:18:07.011163Z","shell.execute_reply.started":"2025-09-29T13:17:57.832701Z","shell.execute_reply":"2025-09-29T13:18:07.010590Z"}},"outputs":[{"name":"stderr","text":"2025-09-29 13:18:03.576881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759151883.599187     376 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759151883.605905     376 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_name = 'dbmdz/bert-base-turkish-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:07.015337Z","iopub.execute_input":"2025-09-29T13:18:07.015557Z","iopub.status.idle":"2025-09-29T13:18:07.729939Z","shell.execute_reply.started":"2025-09-29T13:18:07.015539Z","shell.execute_reply":"2025-09-29T13:18:07.729134Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"ds = load_dataset(\"winvoker/turkish-sentiment-analysis-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:07.730812Z","iopub.execute_input":"2025-09-29T13:18:07.731090Z","iopub.status.idle":"2025-09-29T13:18:09.154783Z","shell.execute_reply.started":"2025-09-29T13:18:07.731062Z","shell.execute_reply":"2025-09-29T13:18:09.154009Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train = ds['train']\ntest = ds['test']\n\ntrain = train.remove_columns('dataset')\ntest = test.remove_columns('dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:09.155562Z","iopub.execute_input":"2025-09-29T13:18:09.155839Z","iopub.status.idle":"2025-09-29T13:18:09.163112Z","shell.execute_reply.started":"2025-09-29T13:18:09.155813Z","shell.execute_reply":"2025-09-29T13:18:09.162481Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def tokenize(batch):\n    label_map = {'Notr': 0, 'Negative': 1, 'Positive': 2}\n    batch['label'] = [label_map[label] for label in batch['label']]\n    return tokenizer(batch['text'], truncation=True, padding=\"max_length\",\n        max_length=128)\n\ntokenized_train = train.map(tokenize, batched=True)\ntokenized_test = test.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:09.163767Z","iopub.execute_input":"2025-09-29T13:18:09.163935Z","iopub.status.idle":"2025-09-29T13:18:14.909558Z","shell.execute_reply.started":"2025-09-29T13:18:09.163922Z","shell.execute_reply":"2025-09-29T13:18:14.908950Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/48965 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196456ea3b834052a37ed6210128af88"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenized_train.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\ntokenized_test.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:14.910359Z","iopub.execute_input":"2025-09-29T13:18:14.910659Z","iopub.status.idle":"2025-09-29T13:18:14.915681Z","shell.execute_reply.started":"2025-09-29T13:18:14.910635Z","shell.execute_reply":"2025-09-29T13:18:14.914938Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    load_acc = evaluate.load('accuracy')\n    load_f1 = evaluate.load('f1')\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = load_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n    return {\"accuracy\": accuracy, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:18:14.916463Z","iopub.execute_input":"2025-09-29T13:18:14.916710Z","iopub.status.idle":"2025-09-29T13:18:15.033407Z","shell.execute_reply.started":"2025-09-29T13:18:14.916687Z","shell.execute_reply":"2025-09-29T13:18:15.032639Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"tr_sent_analysis\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    report_to=\"none\", \n    disable_tqdm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:32:13.332745Z","iopub.execute_input":"2025-09-29T13:32:13.333428Z","iopub.status.idle":"2025-09-29T13:32:13.373562Z","shell.execute_reply.started":"2025-09-29T13:32:13.333406Z","shell.execute_reply":"2025-09-29T13:32:13.372892Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_376/3267386464.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:32:13.460132Z","iopub.execute_input":"2025-09-29T13:32:13.460340Z","iopub.status.idle":"2025-09-29T15:23:15.964059Z","shell.execute_reply.started":"2025-09-29T13:32:13.460323Z","shell.execute_reply":"2025-09-29T15:23:15.962984Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14960' max='41316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14960/41316 1:51:01 < 3:15:36, 2.25 it/s, Epoch 1.09/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.079200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.056400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.044000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.058900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.043700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.050600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.045400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.052000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.044700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.044900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.040100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.027300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.044000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.042700</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.045200</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.131400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.113800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.112600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.110400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.126800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.110500</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.104100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.115000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.107600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.102500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.109000</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.089100</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.097700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.093900</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.112700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.092900</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.090900</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.087100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.108100</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.095200</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.091300</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.109000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.093000</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.104400</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.092700</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.105600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.097700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.094100</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.092200</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.093100</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.101400</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.090300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.078400</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.078800</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.095700</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.099500</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.090500</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.095400</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.076500</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.083700</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.092200</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.102200</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.090200</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.086200</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.090600</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.097600</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.086600</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.097900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.098700</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.089800</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.080600</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.085100</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.098700</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.074500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.088700</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.078300</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.097700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.097000</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.077700</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.091100</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.087600</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.087300</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.085000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.074100</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.102800</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.073600</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.094900</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.082800</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.077600</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.060600</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.091900</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.084000</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.092700</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.072800</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.053300</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.053900</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.073000</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.067100</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.050200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.051900</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.054000</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.059200</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.063900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_376/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"!zip /kaggle/working/tr_sent_analysis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:23:35.151578Z","iopub.execute_input":"2025-09-29T15:23:35.151863Z","iopub.status.idle":"2025-09-29T15:23:35.348988Z","shell.execute_reply.started":"2025-09-29T15:23:35.151842Z","shell.execute_reply":"2025-09-29T15:23:35.348016Z"}},"outputs":[{"name":"stdout","text":"\nzip error: Nothing to do! (/kaggle/working/tr_sent_analysis.zip)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizer\n\ncheckpoint_path = \"/kaggle/working/tr_sent_analysis/checkpoint-13772\"  # son kaydettiğin checkpoint\n\n# Tokenizer yükle\ntokenizer = BertTokenizer.from_pretrained(checkpoint_path)\n\n# Modeli yükle\nmodel = BertForSequenceClassification.from_pretrained(checkpoint_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:25:29.564045Z","iopub.execute_input":"2025-09-29T15:25:29.564578Z","iopub.status.idle":"2025-09-29T15:25:29.688024Z","shell.execute_reply.started":"2025-09-29T15:25:29.564553Z","shell.execute_reply":"2025-09-29T15:25:29.687486Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"text = \"Bu ürün çok hoştu ama ufak tefek sıkıntıları var\"\ntokenized_text = tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=\"max_length\")\ntokenized_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:33:19.266806Z","iopub.execute_input":"2025-09-29T15:33:19.267381Z","iopub.status.idle":"2025-09-29T15:33:19.274846Z","shell.execute_reply.started":"2025-09-29T15:33:19.267358Z","shell.execute_reply":"2025-09-29T15:33:19.274185Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    2,  2123,  2782,  2140,  4008,  2598,  2262,  6770, 27681, 22646,\n          2166,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"type(tokenized_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:33:19.618329Z","iopub.execute_input":"2025-09-29T15:33:19.618832Z","iopub.status.idle":"2025-09-29T15:33:19.623479Z","shell.execute_reply.started":"2025-09-29T15:33:19.618806Z","shell.execute_reply":"2025-09-29T15:33:19.622767Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"transformers.tokenization_utils_base.BatchEncoding"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    res = model(**tokenized_text)  # res bir SequenceClassifierOutput\n\nlogits = res.logits  # logits tensoru al\npredicted_class_id = torch.argmax(logits, dim=-1).item()  # tensor üzerinde argmax\n\nid2label = {0: \"Notr\", 1: \"Negative\", 2: \"Positive\"}\nprint(\"Tahmin:\", id2label[predicted_class_id])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:33:20.089996Z","iopub.execute_input":"2025-09-29T15:33:20.090692Z","iopub.status.idle":"2025-09-29T15:33:20.268255Z","shell.execute_reply.started":"2025-09-29T15:33:20.090667Z","shell.execute_reply":"2025-09-29T15:33:20.267492Z"}},"outputs":[{"name":"stdout","text":"Tahmin: Positive\n","output_type":"stream"}],"execution_count":48}]}