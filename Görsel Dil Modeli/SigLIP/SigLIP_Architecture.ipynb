{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "YAMxhdrfR9C_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, embed_dim: int = 768, image_size: int = 224):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "            padding=0\n",
        "        )\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.position_embedding = nn.Embedding(num_embeddings=num_patches, embedding_dim=embed_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      # (batch_size, 3, 224, 224) -> (batch_size, 768, 14, 14)\n",
        "        x = self.patch_embedding(x)\n",
        "        # -> (batch_size, 768, 196) -> (batch_size, 196, 768)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Pozisyon embedding ekle\n",
        "        positions = torch.arange(x.size(1), device=x.device)\n",
        "        position_embeddings = self.position_embedding(positions)\n",
        "        x = x + position_embeddings\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SigLIP modeli CLIP modelinin gelişmiş halidir. Bu modeller metin ve görselleri aynı temsil uzayında birleştirerek çoklu modellerin önünü açtı. Ancak CLIP modelinin bazı zayıf yönleri vardı. \n",
        "\n",
        "Contrastive loss (karşılaştırmalı kayıp) yapısı, yalnızca “pozitif” ve “negatif” örnekler arasındaki farkı öğreniyordu. Bu, küçük batch boyutlarında zayıf performans ve dengesiz gradyan akışı gibi sorunlara yol açıyordu.\n",
        "\n",
        "Google Research, bu eksikleri düzeltmek için SigLIP (Sigmoid Loss for Language-Image Pretraining) adlı modeli tanıttı.\n",
        "\n",
        "\n",
        "SigLIP, CLIP ile aynı mimari temele sahiptir. Bir görsel encoder (ViT veya benzeri) ve bir text encoder. Fakat asıl fark öğrenme fonksiyonundadır. CLIP, görüntü ve metin embedding’lerini normalize edip, tüm batch boyunca bir contrastive cross-entropy loss hesaplar. SigLIP ise sigmoid tabanlı bir binary loss kullanır. Bu değişiklik, modelin davranışında büyük fark yaratır. Yani SigLIP, CLIP’in “karşılaştırmalı” düşünme biçimini bırakıp her görsel-metin çiftini ayrı bir ikili sınıflandırma problemi olarak ele alır.\n",
        "\n",
        "Gelin bu mimarideki görsel encoder'ı ele alalım. Bunun için öncelikli olarak orijinal modeli sisteme kuralım."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urrm8tA4b69_",
        "outputId": "95b3409d-572d-48aa-ebd2-2141451b3da1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SiglipVisionModel(\n",
              "  (vision_model): SiglipVisionTransformer(\n",
              "    (embeddings): SiglipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
              "      (position_embedding): Embedding(196, 768)\n",
              "    )\n",
              "    (encoder): SiglipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x SiglipEncoderLayer(\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attn): SiglipAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SiglipMLP(\n",
              "            (activation_fn): GELUTanh()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (head): SiglipMultiheadAttentionPoolingHead(\n",
              "      (attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): SiglipMLP(\n",
              "        (activation_fn): GELUTanh()\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoProcessor, SiglipVisionModel, SiglipVisionConfig\n",
        "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\", config = SiglipVisionConfig(vision_use_head=True))\n",
        "vision_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Görüldüğü üzere MLP yapısı klasik LLM modellerine benzemektedir. Aynı şekilde Attention yapısı da klasik mimariye benzemektedir. Bu yaklaşımları ve mimariyi incelemek için GPT2, BERT, Gemma modelleri ile ilgili oluşturduğum notebookları inceleyebilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "pXBO2TPAUWGR"
      },
      "outputs": [],
      "source": [
        "class SiglipAttention(nn.Module):\n",
        "  def __init__(self, embed_dim:int = 768, n_heads:int=8) -> None:\n",
        "    super().__init__()\n",
        "    assert embed_dim % n_heads == 0, \"embed_dim, n_heads'e tam bölünmelidir...\"\n",
        "    self.d_head = embed_dim // n_heads\n",
        "    self.n_heads = n_heads\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.n_heads * self.d_head, bias = True)\n",
        "    self.k_proj = nn.Linear(self.embed_dim, self.n_heads * self.d_head, bias = True)\n",
        "    self.v_proj = nn.Linear(self.embed_dim, self.n_heads * self.d_head, bias = True)\n",
        "    self.out_proj = nn.Linear(self.n_heads * self.d_head, self.embed_dim, bias = True)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, seq_len, dim = x.shape\n",
        "    # (batch_size, seq_len, dim) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
        "    q = self.q_proj(x).transpose(1, 2)\n",
        "    k = self.k_proj(x).transpose(1, 2)\n",
        "    v = self.v_proj(x).transpose(1, 2)\n",
        "    # (batch_size, n_heads, seq_len, d_head) @ (batch_size, n_heads, d_head, seq_len) -> (batch_size, n_heads, seq_len, seq_len)\n",
        "    attn_scores = q @ k.transpose(-1, -2)\n",
        "    attn_scores = attn_scores / (self.d_head ** 0.5)\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "    # (batch_size, n_heads, seq_len, seq_len) @ (batch_size, n_heads, seq_len, d_head) -> (batch_size, n_heads, seq_len, d_head)\n",
        "    attn_output = attn_probs @ v\n",
        "    # (batch_size, n_heads, seq_len, d_head) -> (batch_size, seq_len, n_heads, d_head) -> (batch_size, seq_len, n_heads * d_head)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.n_heads * self.d_head)\n",
        "    # (batch_size, seq_len, n_heads * d_head) -> (batch_size, seq_len, dim)\n",
        "    out_proj = self.out_proj(attn_output)\n",
        "    return out_proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "aQuY8b0Nc5oR"
      },
      "outputs": [],
      "source": [
        "class GELUTanh(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * x * (1.0 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "                                            (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 768, hidden_dim: int = 3072) -> None:\n",
        "        super().__init__()\n",
        "        self.activation_fn = GELUTanh()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim, bias = True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim, bias = True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_fn(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "GUHcw0xLcoMZ"
      },
      "outputs": [],
      "source": [
        "class SiglipEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim:int = 768, n_heads:int = 8, n_layers:int=12) -> None:\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim, eps = 1e-06, elementwise_affine= True)\n",
        "    self.self_attn = SiglipAttention(embed_dim, n_heads)\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim, eps = 1e-06, elementwise_affine= True)\n",
        "    self.mlp = SiglipMLP(embed_dim)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    residual = x\n",
        "    x = self.layer_norm1(x)\n",
        "    x = self.self_attn(x)\n",
        "    residual = x\n",
        "    x = x + residual\n",
        "    x = self.layer_norm2(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ey60HjqAldZB"
      },
      "outputs": [],
      "source": [
        "class SiglipEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim: int = 768, n_heads: int = 8, hidden_dim: int = 3072, n_layers: int = 12) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "      SiglipEncoderLayer(embed_dim, n_heads, hidden_dim) for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "bpb8-nVfm642"
      },
      "outputs": [],
      "source": [
        "class SiglipMultiHeadAttentionPoolingHead(nn.Module):\n",
        "  def __init__(self, embed_dim:int =768, n_heads:int = 8, hidden_dim:int = 3072) -> None:\n",
        "    super().__init__()\n",
        "    self.attention = nn.MultiheadAttention(embed_dim = embed_dim, num_heads = n_heads, bias = True)\n",
        "    self.out_proj = nn.Linear(embed_dim, embed_dim, bias = True)\n",
        "    self.layernorm = nn.LayerNorm(embed_dim, eps = 1e-06, elementwise_affine= True)\n",
        "    self.mlp = SiglipMLP(embed_dim, hidden_dim)\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    x = self.attention(x, x, x)[0]\n",
        "    x = self.out_proj(x)\n",
        "    x = self.layernorm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "xi-vkEh9hVzg"
      },
      "outputs": [],
      "source": [
        "class SiglipVisionTransformer(nn.Module):\n",
        "  def __init__(self, embed_dim:int = 768, n_heads:int = 8, n_layers:int = 12, hidden_dim:int = 3072) -> None:\n",
        "    super().__init__()\n",
        "    self.embeddings = SiglipVisionEmbeddings()\n",
        "    self.encoder = SiglipEncoder(embed_dim, n_heads, hidden_dim, n_layers)\n",
        "    self.post_layernorm = nn.LayerNorm(embed_dim, eps = 1e-06, elementwise_affine= True)\n",
        "    self.head = SiglipMultiHeadAttentionPoolingHead(embed_dim, n_heads, hidden_dim)\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    x = self.embeddings(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.post_layernorm(x)\n",
        "    x = self.head(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "u1S4DPudoeyL"
      },
      "outputs": [],
      "source": [
        "class SiglipVisionModel(nn.Module):\n",
        "  def __init__(self, embed_dim:int = 768, n_heads:int = 8, n_layers:int = 12, hidden_dim:int = 3072) -> None:\n",
        "    super().__init__()\n",
        "    self.vision_model = SiglipVisionTransformer(embed_dim, n_heads, n_layers, hidden_dim)\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    x = self.vision_model(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtOreRm8o7mY",
        "outputId": "8708b00b-87f2-4daa-cb5e-ecaef5c754a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SiglipVisionModel(\n",
              "  (vision_model): SiglipVisionTransformer(\n",
              "    (embeddings): SiglipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (position_embedding): Embedding(196, 768)\n",
              "    )\n",
              "    (encoder): SiglipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x SiglipEncoderLayer(\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attn): SiglipAttention(\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SiglipMLP(\n",
              "            (activation_fn): GELUTanh()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (head): SiglipMultiHeadAttentionPoolingHead(\n",
              "      (attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): SiglipMLP(\n",
              "        (activation_fn): GELUTanh()\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SiglipVisionModel()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpzJCCDbqJOR",
        "outputId": "10efa732-962b-4113-c6b8-333f88db2695"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.0449, -0.0932, -0.2317,  ..., -0.1659, -0.3987, -0.3212],\n",
              "         [-0.0445, -0.0932, -0.2318,  ..., -0.1658, -0.3991, -0.3215],\n",
              "         [-0.0450, -0.0933, -0.2316,  ..., -0.1659, -0.3985, -0.3212],\n",
              "         ...,\n",
              "         [-0.0443, -0.0928, -0.2317,  ..., -0.1657, -0.3995, -0.3214],\n",
              "         [-0.0441, -0.0926, -0.2318,  ..., -0.1657, -0.3999, -0.3215],\n",
              "         [-0.0448, -0.0935, -0.2318,  ..., -0.1659, -0.3986, -0.3214]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rand_tensor = torch.rand(1, 3, 224, 224)\n",
        "model(rand_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxWo0JuKrOsk",
        "outputId": "cd87c20f-342b-4e0b-8c8a-dc3422cdc463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(rand_tensor).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MixgTNFwMUqx",
        "outputId": "a360dde9-dec6-4cf8-c4a4-91392fc85733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.5459,  2.4697,  0.0855,  ..., -0.9363,  1.1687,  0.2339],\n",
              "         [-0.6831,  1.3061, -3.3035,  ..., -0.0148,  0.4427, -0.1520],\n",
              "         [-0.9031, -0.6246,  1.1616,  ..., -0.7311,  0.5455,  0.8208],\n",
              "         ...,\n",
              "         [-0.8822, -0.9972, -2.1969,  ...,  0.8250,  0.0052, -0.5779],\n",
              "         [ 0.4814,  1.7879, -0.2659,  ..., -1.5753,  1.8970, -1.1469],\n",
              "         [-0.3724,  0.0985,  0.8736,  ..., -1.8435,  0.4599,  0.9571]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.8030e-01, -1.5814e-01, -8.7938e-03, -1.2028e-01, -1.2303e-02,\n",
              "          1.0454e-01, -9.6347e-01, -4.1511e-01,  6.9938e-02,  3.9612e-01,\n",
              "          3.0890e-01,  6.0258e-03,  1.6329e-01, -2.3422e-01,  2.6406e-01,\n",
              "         -4.4966e-01, -2.6904e-01, -1.2774e-01,  6.9778e-01, -1.9921e-01,\n",
              "         -4.1621e-02, -2.1980e-01, -2.2761e-01, -3.1168e-01,  8.9810e-03,\n",
              "          3.8619e-02,  3.4041e-01,  2.2537e-01,  2.7722e-01,  3.7360e-01,\n",
              "         -2.1578e-01, -2.4395e-02, -2.2273e-01,  1.5484e-01,  2.3911e-01,\n",
              "         -1.3560e-01, -2.3661e-01, -1.1568e-01,  3.8206e-01,  4.7294e-01,\n",
              "         -6.0108e-01, -9.7940e-02, -9.6640e-03,  8.7740e-02, -2.5483e-02,\n",
              "         -5.6273e-01,  5.7591e-02, -4.3509e-01,  2.4833e-01, -6.3734e-02,\n",
              "          7.1759e-01, -5.2513e-01, -1.9644e-01, -2.2165e-01,  4.3143e-01,\n",
              "         -4.7857e-01,  4.6739e-01,  2.2146e-01, -4.2968e-02, -2.0463e-01,\n",
              "         -8.0599e-02,  3.9054e-01,  2.5477e-01,  1.1585e-01,  3.0617e-01,\n",
              "          3.3750e-01,  6.8879e-01, -4.2033e-02,  4.0641e-02,  2.0049e-01,\n",
              "          5.2373e-02,  1.9368e-01, -2.2168e-01,  1.2736e-01,  2.0850e-01,\n",
              "         -2.5871e-01,  2.1824e-01,  2.7831e-01,  7.5755e+00, -4.0768e-01,\n",
              "          1.1902e-01, -7.1780e-01, -2.2216e-02,  6.3954e-01,  7.1775e-02,\n",
              "          9.1569e-01,  2.9050e-01, -7.7727e-01,  1.9219e-01,  2.4492e-01,\n",
              "         -2.6586e-01, -1.4606e-01,  4.7223e-01, -1.0919e-01,  9.7472e-01,\n",
              "          1.0133e-01, -6.7603e-01,  1.0503e-01,  1.7862e-01,  1.8007e-01,\n",
              "          1.4977e-01,  4.1691e-01,  2.5815e-01,  3.2703e-01,  5.6057e-01,\n",
              "         -1.1117e-01, -7.0193e-01,  1.5016e-01,  3.7747e-01,  5.6579e-01,\n",
              "         -9.5287e-02, -1.2408e-02, -5.7145e-03, -1.0368e-01, -4.1984e-01,\n",
              "         -1.8309e-01, -1.6695e-01,  1.1733e-01, -2.4171e-01,  3.4701e-01,\n",
              "         -1.7409e-01,  8.7286e-01,  4.8418e-01, -2.8160e-01, -3.2826e-01,\n",
              "          3.6359e-01, -2.5303e-01, -8.9087e-02,  1.1359e+00,  3.5800e-01,\n",
              "         -1.5319e-02,  1.2000e-01,  3.8663e-01, -3.0832e-02, -1.0952e-01,\n",
              "          9.4392e-02,  1.0494e-01,  1.4866e-01,  2.4525e-01, -2.7469e-01,\n",
              "         -1.2976e-01,  8.5343e-01, -7.1379e-02, -1.3261e-02,  1.8192e+00,\n",
              "         -2.7664e-01,  2.1743e-02, -2.6403e-01,  4.9615e-01, -4.9445e-01,\n",
              "          7.1221e-02, -2.8275e-02,  2.8356e-01,  4.9230e+00,  7.6326e-02,\n",
              "         -2.4494e-02, -1.9126e-01,  5.1013e-01, -2.5337e-01, -1.6800e-01,\n",
              "         -3.9734e-02, -1.4825e-01,  3.6825e-01, -5.2298e-01, -1.1309e-01,\n",
              "          8.2806e-02,  4.0999e-02,  1.5320e-02,  1.5557e-01, -6.4237e-02,\n",
              "          8.9208e-02, -1.1371e-01, -7.7106e-01,  5.4751e-02, -9.1697e-01,\n",
              "         -3.9703e-01, -8.1571e-01, -5.2208e-01, -3.7606e-01, -2.5612e-01,\n",
              "         -2.7651e-01,  1.1768e-01, -3.7638e-02, -2.1916e-01, -8.6432e-02,\n",
              "          5.5212e-01,  7.7853e-02,  3.4911e-01,  3.4307e-01,  5.1080e-01,\n",
              "          2.4386e-03,  5.0577e-01,  2.4969e-01,  2.3178e-01, -8.7290e-02,\n",
              "         -3.2794e-02, -1.2819e+00,  1.0197e-01, -5.0646e-02, -8.0340e-02,\n",
              "         -1.0596e-01,  2.7815e-01, -1.5389e-01, -2.8015e-02,  1.0528e-01,\n",
              "          1.9219e-01,  6.7845e-02,  1.1886e+00, -3.8082e-01,  3.5788e-01,\n",
              "         -6.6383e-01,  7.0019e-01,  8.9309e-02, -9.1380e-01,  3.6767e-02,\n",
              "          2.5475e-01, -5.6212e-01,  5.3618e-01, -1.3261e-01,  6.3071e-01,\n",
              "          2.2420e-01, -1.3363e-01, -2.2541e-01,  8.6542e-02, -3.0763e-01,\n",
              "          7.4952e-01,  1.1595e-03,  1.8189e-01, -4.4046e-02, -2.8635e-01,\n",
              "          8.2209e-02, -6.4576e-03,  2.0210e-01,  2.7673e-03,  3.7143e-01,\n",
              "         -2.0652e-01, -2.3965e-01, -3.8735e-01,  8.6732e-02,  1.2794e-01,\n",
              "         -1.3213e-01, -4.7286e-02,  5.2063e-01,  4.1672e-02, -6.4321e-02,\n",
              "          1.1434e-01, -4.1349e-01, -1.4496e-01,  8.5002e-02,  3.4030e-01,\n",
              "         -3.7662e-02,  9.8635e-02,  6.7507e-02, -2.6260e-01, -1.6215e-01,\n",
              "         -2.3906e-01,  1.7766e-01, -2.4295e-01,  6.5031e-01, -6.7547e-01,\n",
              "         -1.2422e+00, -8.1585e-02, -6.8947e-03,  3.7047e-01, -3.9363e-02,\n",
              "         -7.5414e-02,  8.9950e-01,  1.9015e-01, -1.9346e-01, -7.0368e-02,\n",
              "         -4.5863e-01,  2.6662e-04,  8.4408e-01, -2.3717e-01,  1.8994e-01,\n",
              "         -4.8491e-01, -7.3007e-01, -4.6303e-02,  6.6510e-03,  6.8889e-02,\n",
              "         -1.0641e-02,  4.4944e-01, -4.2315e-02, -5.6186e-01,  2.0766e-01,\n",
              "         -8.2602e-02,  4.7728e-01, -3.4432e-01, -1.5253e-01, -5.1001e-02,\n",
              "         -3.8213e-01,  6.7966e-02,  1.0782e-01, -5.7652e-01, -3.0361e-01,\n",
              "         -3.5212e-01, -2.6855e-01, -2.4956e-01, -9.3706e-02, -3.0370e-01,\n",
              "         -7.9576e-01,  4.4336e+00, -4.7117e-02, -3.2016e-03, -4.1230e-01,\n",
              "          1.0594e-01,  2.3679e-01, -4.4377e-01,  5.0111e-01, -2.2575e-01,\n",
              "         -2.4097e-01,  3.4559e-01, -2.0909e-01,  2.8285e-01, -3.5326e-01,\n",
              "         -3.1098e-01, -2.0673e-02,  7.0612e-01, -1.0200e-01,  4.6252e-01,\n",
              "          2.6242e-01, -1.1189e-01,  1.5719e-01,  1.5453e-01,  6.1523e-01,\n",
              "          2.2012e-01,  1.8517e-01,  6.2052e-02, -1.8840e-01, -3.9279e-01,\n",
              "          5.7995e-01,  1.0620e-02, -2.5735e-01, -2.5918e-01, -8.6584e-02,\n",
              "         -4.2583e-01, -1.4498e-01, -3.0619e-01,  1.5039e-01,  1.7802e-01,\n",
              "          4.7858e-01,  4.9545e-01, -3.1914e-01,  1.7543e-01, -4.5324e-01,\n",
              "         -4.0363e-01,  1.7472e-01,  4.3666e-01, -8.6283e-01,  3.3214e-01,\n",
              "         -3.0167e-01, -4.5899e-04, -3.3390e-01, -3.9466e-01, -4.6733e-01,\n",
              "         -7.6233e-02,  5.0481e-02, -2.2856e-01,  2.7565e-01,  4.6430e-01,\n",
              "          2.0906e-01, -3.0065e-01,  8.7351e-02, -2.0008e-01, -1.9400e-01,\n",
              "         -2.4698e-01, -9.6137e-02, -2.9464e-01,  2.7607e-01, -3.4579e-02,\n",
              "         -7.3715e-02,  1.6472e-02,  1.1696e-01,  8.1471e-02,  7.3391e-01,\n",
              "          2.1799e-02, -1.4051e-01,  3.7185e-01, -1.0792e-02,  2.4540e-01,\n",
              "         -7.7212e-01, -2.8326e-02, -9.8828e-01, -1.9752e-01,  7.1859e-02,\n",
              "          1.3204e-01, -3.4137e-02,  2.7027e-01, -1.0308e-01,  2.6660e-01,\n",
              "          5.0746e-01, -1.2679e-02,  2.9690e-01,  4.5088e-01,  9.7746e-01,\n",
              "         -1.5631e-01, -4.6534e-02,  6.3909e-02, -3.3794e-01, -5.5613e-01,\n",
              "          1.7199e-01,  4.4800e-02,  4.5648e-02,  7.3060e-01,  1.1830e+00,\n",
              "          1.2606e-01, -9.2940e-02, -7.9733e-02, -1.3070e-01,  1.6770e-01,\n",
              "         -5.2321e-01,  2.8398e-01, -2.7612e-01, -2.9918e-01,  2.8510e-01,\n",
              "         -7.4953e-01,  2.7768e-01, -2.8892e-01, -1.2158e-01,  6.6896e-02,\n",
              "          3.3305e-02,  1.0388e-01,  1.2781e-01, -4.9071e-02,  5.6224e-01,\n",
              "         -1.4617e-01,  8.2168e-02, -2.5546e-01, -1.5820e+00,  5.4637e-01,\n",
              "          2.9710e-01,  3.6327e-01,  2.2765e-01, -2.0663e-01,  1.6689e-01,\n",
              "          1.9423e-01, -4.6792e-02,  7.0306e-02, -7.0079e-02,  1.2827e-02,\n",
              "          1.1939e-02,  6.6361e-01, -3.1351e-01,  6.0663e-01, -2.2117e-02,\n",
              "          1.6804e-01,  1.4483e-01, -2.0989e-01, -3.1176e-01, -7.7190e-01,\n",
              "          1.0419e-01, -4.1687e-01,  2.3801e-01,  6.0677e-01,  2.4524e-01,\n",
              "          4.7297e-01,  1.1223e-02, -2.1566e-01, -2.7722e-01,  1.9096e-01,\n",
              "         -1.2740e-01, -2.7285e-01, -4.0593e-01,  2.0483e-01, -1.3517e-01,\n",
              "         -6.4596e-02,  4.7386e-01, -1.9991e-02, -5.5242e-01,  2.9984e-02,\n",
              "          6.3043e-02,  3.3850e-01,  8.8171e-02, -6.4369e-02,  2.8003e-01,\n",
              "          8.8257e-02,  7.5889e-02, -5.7331e-02, -2.8789e-01, -4.9821e-01,\n",
              "          4.8102e-02,  1.0669e-01, -5.8212e-01, -4.6259e-01,  1.3408e-01,\n",
              "          4.7074e-01, -4.0295e-02, -1.0871e-01, -1.6926e-01,  1.6898e-01,\n",
              "         -1.3850e-01, -2.0623e-01,  1.0840e-01,  4.1791e-01, -1.6811e-01,\n",
              "          1.7359e-01,  1.0161e-01, -6.1161e-02, -9.7271e-01,  1.0870e-01,\n",
              "          2.3856e-01,  2.3485e-01, -6.8680e-02,  9.5263e-02, -2.0722e-01,\n",
              "          8.4949e-03, -1.9123e-01,  1.3602e-01,  6.8315e-02, -2.2036e-01,\n",
              "          7.4184e-01, -2.8051e-01,  2.4664e-01, -2.6055e-03, -1.7260e-01,\n",
              "          1.2317e-01, -2.6017e-01,  6.7512e-01, -2.8633e-01,  5.8437e-01,\n",
              "         -1.7199e-01, -7.0902e-02,  1.3425e-01, -2.3501e+00, -3.3726e-01,\n",
              "          2.8604e-01,  2.1386e-01,  4.5398e-02,  3.9062e-01,  2.1747e-01,\n",
              "         -8.3974e-01, -5.5028e-02,  2.0722e-01,  1.1622e+00, -5.9387e-02,\n",
              "          2.4350e+00,  2.4830e-01, -1.3543e-01,  6.4363e-01,  2.0769e-02,\n",
              "         -3.3753e-02,  2.1305e-02,  5.8865e-01,  3.4608e-01, -4.9784e-02,\n",
              "          4.0341e-01,  6.3789e-02, -2.2714e-01,  1.0277e-01, -5.0587e-01,\n",
              "         -1.4419e-01, -4.5717e-01,  2.6570e-01, -2.4172e-01,  2.8043e-01,\n",
              "          1.1308e-01,  3.3144e-03,  8.3928e-01,  5.4210e-01, -2.2636e-02,\n",
              "         -1.0717e+00,  1.6398e-01, -2.4669e-01, -2.0418e-01,  2.2850e-01,\n",
              "          2.3528e-01, -5.6531e-02,  7.2624e-02,  3.5705e-02,  5.3944e-02,\n",
              "         -1.4497e+00,  3.6325e-02,  9.4153e-02, -1.2742e-01, -1.8603e-01,\n",
              "         -6.7075e-01, -1.0521e-01, -2.3580e-01,  1.0883e-01,  7.4990e-03,\n",
              "         -2.2708e-01, -6.3968e-01,  3.5370e-02,  8.3043e-02, -1.6206e-01,\n",
              "          5.7548e-01, -1.5306e-02,  5.7796e-01, -1.0599e-01, -3.8300e-01,\n",
              "         -1.1384e-01, -6.3585e-01, -8.9490e-03,  4.1919e-01,  3.0969e-01,\n",
              "         -3.4201e-01, -1.6818e+00, -3.9273e-02,  1.7238e-02, -1.7705e-01,\n",
              "         -3.9969e-02,  3.7073e-01, -5.5579e-01, -1.9364e-01,  1.6782e-02,\n",
              "         -6.6209e-01, -4.9265e-01,  1.0950e-01,  4.1609e-01,  1.3695e+00,\n",
              "          7.6524e-02,  5.0352e-01, -3.5702e-01,  3.0946e-01, -2.1927e-02,\n",
              "          1.4571e-01,  4.6543e-01,  2.3023e-01, -2.8452e-01,  1.7348e-02,\n",
              "          2.0546e-01,  3.5883e-01, -3.0761e-01, -5.1874e-01, -4.7667e-01,\n",
              "          3.9573e-02, -2.8578e-01,  2.5628e-01, -2.4952e-01, -7.3012e-01,\n",
              "         -5.5007e-02,  3.8963e-02, -2.8250e+00, -4.1223e-02,  7.0590e-02,\n",
              "         -2.6316e-01,  9.0922e-01, -1.8638e-01, -1.2883e-01,  2.4302e-01,\n",
              "         -2.6806e-01, -7.1085e-02, -3.8755e-02,  4.6989e-01, -1.5344e-01,\n",
              "         -7.3418e-01, -1.2788e+00,  2.2742e-01,  9.3234e-01, -4.1767e-01,\n",
              "          2.3321e+00,  1.7914e-01, -1.1024e-01,  5.1619e-01,  1.0408e-01,\n",
              "         -1.8284e-01,  1.1041e+00,  6.5629e-03, -4.6229e-01, -1.0128e-01,\n",
              "         -4.2351e-01,  4.0329e-01,  6.8041e-02, -2.9277e-01,  4.1645e-02,\n",
              "         -9.4614e-01, -7.7630e-01, -1.0555e-01,  2.8315e-02,  3.3084e-01,\n",
              "         -9.0960e-02,  1.6261e-01,  9.8700e-02,  1.1968e+00,  1.9248e-01,\n",
              "          1.8622e-01, -2.0948e-01, -1.6110e-01,  2.1572e-01,  2.6317e-01,\n",
              "          8.7988e-04, -1.3878e-01, -1.9422e-01,  2.9369e-01,  1.2353e+00,\n",
              "          7.4303e-01, -1.3296e-01, -9.3377e-01,  3.1191e-01,  4.3996e-02,\n",
              "         -6.8304e-02,  2.0244e-01, -2.8827e-01, -4.4621e-01,  2.4083e-01,\n",
              "          6.2594e-01, -1.6778e-01, -5.5922e-01,  1.7687e-01,  3.1894e-01,\n",
              "          8.1156e-01,  8.8597e-02,  8.4352e-02, -2.0695e-01,  5.2940e-01,\n",
              "          3.0686e-02, -3.3998e-01, -1.2252e-01,  5.1453e-02,  1.2065e-01,\n",
              "         -3.8842e-01, -3.8426e-01, -8.3428e-01, -1.9854e-01,  1.2252e-01,\n",
              "         -1.4388e+00,  2.9468e-01,  9.2971e-02, -4.3499e-01,  8.5332e-02,\n",
              "         -7.4345e-01, -1.6618e-01, -1.9936e-01,  5.4819e-03,  7.5938e-02,\n",
              "          3.5689e-01,  5.0185e-02,  1.3554e-01,  1.1621e+00, -1.3217e+00,\n",
              "         -2.2992e-01, -3.4627e-01,  1.7439e-01,  2.8629e-01, -8.3783e-03,\n",
              "         -1.3184e-01,  2.0894e-02, -1.7229e-01, -1.0096e-03,  1.6589e-01,\n",
              "         -4.6899e-01, -1.6851e-02, -1.1943e-01,  1.8593e-01,  4.9771e-02,\n",
              "          3.5504e-01, -8.3477e-02, -1.2976e+00, -2.0851e-01,  4.4738e-02,\n",
              "          3.9257e-01, -6.9444e-02,  6.3425e-02, -4.1933e-03, -3.5502e-01,\n",
              "          1.4734e-01, -5.4382e-01, -3.5238e-01, -5.5978e-01,  7.6821e+00,\n",
              "         -2.9106e-01,  3.2208e-02, -2.9145e-01, -6.9221e-01,  2.6929e-01,\n",
              "         -1.9241e-01,  3.2421e-01, -2.3184e-02]], grad_fn=<SelectBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vision_model(rand_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7ynAyV-rVAE",
        "outputId": "5d99af14-f868-4de3-cef7-7b2c818d65e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 196, 768])\n",
            "torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "output = vision_model(rand_tensor)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(output.pooler_output.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
