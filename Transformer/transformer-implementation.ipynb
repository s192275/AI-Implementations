{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aZDjIiTE4Zm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEyh1XrTFvcw"
      },
      "outputs": [],
      "source": [
        "class ImputEmbeddings(nn.Module):\n",
        "  def __init__(self, vocab_size:int, d_model:int) -> None:\n",
        "    super(ImputEmbeddings, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    self.d_model = d_model\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    return self.embed(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuFqmPNgHJiS"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.pe = torch.zeros(seq_len, d_model)\n",
        "    self.sentences = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    self.div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    self.pe[:,1::2] = torch.cos(self.sentences / self.div_term)\n",
        "    self.pe[:,0::2] = torch.sin(self.sentences / self.div_term)\n",
        "    #Bunu şöyle düşün girdi batch_size, seq_len, d_model kadar olacak pe seq_len, d_model kadar. Bu yüzden buna bir batch ekle\n",
        "    self.pe = self.pe.unsqueeze(0)\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    x = x + self.pe[:, :x.size(1), :].detach() #Burada pe parametresi öğrenilmeyen sabit bir ifadedir.\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_PfVL2_Jtlm"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model:int, d_ff:int, dropout:float) -> None:\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    return self.fc2(self.dropout(self.relu(self.fc1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc_8jzFJMtH9"
      },
      "outputs": [],
      "source": [
        "class AddNormLayer(nn.Module):\n",
        "  def __init__(self, eps:int = 10**-6) -> None:\n",
        "    super(AddNormLayer, self).__init__()\n",
        "    self.alpha = nn.Parameter(torch.tensor(1.0))\n",
        "    self.beta = nn.Parameter(torch.tensor(1.0))\n",
        "    self.eps = eps\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    mean = x.mean(-1, keepdim = True)\n",
        "    std = x.std(-1, keepdim = True)\n",
        "    out = self.alpha * (x - mean) / (std + self.eps) + self.beta\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMlAU8Z9JxZl"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout:float) -> None:\n",
        "    super(ResidualConnection, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = AddNormLayer()\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(self.norm(sublayer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTigS6NZP4fT"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model:int, h:int, dropout:float) -> None:\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = d_model // h\n",
        "    assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "    self.out_linear = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, q, k, v, mask) ->torch.Tensor:\n",
        "    key = self.w_k(k) #(batch_size, seq_len, d_model)\n",
        "    value = self.w_v(v) #(batch_size, seq_len, d_model)\n",
        "    query = self.w_q(q) #(batch_size, seq_len, d_model)\n",
        "    #(batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    #(batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    #(batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    #(batch_size, h, seq_len, d_k) * (batch_size, h, d_k, seq_len) -> (batch_size, h, seq_len, seq_len)\n",
        "    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention_weights = F.softmax(scores)\n",
        "    #(batch_size, h, seq_len, seq_len) * (batch_size, h, seq_len, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    attended = torch.matmul(attention_weights, value)\n",
        "    #(batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, d_model) ->(batch_size, seq_len, d_model)\n",
        "    attended = attended.transpose(1, 2).contiguous().view(attended.shape[0], -1, self.d_model)\n",
        "    output = self.out_linear(attended)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK5h8fYcPw3x"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int) -> None:\n",
        "    super(Linear, self).__init__()\n",
        "    self.linear = nn.Linear(d_model, vocab_size)\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhWDEZrmwrFW"
      },
      "outputs": [],
      "source": [
        "class Softmax(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int) -> None:\n",
        "    super(Softmax, self).__init__()\n",
        "    self.softmax = nn.Softmax(dim = -1)\n",
        "  def forward(self, x) ->torch.Tensor:\n",
        "    return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkxQUwqQiZuI"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model:int, h:int, d_ff:int, dropout:float) -> None:\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    self.add_norm = AddNormLayer()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "    self.add_norm2 = AddNormLayer()\n",
        "  def forward(self, x, mask) ->torch.Tensor:\n",
        "    x = self.attention(x, x, x, mask)\n",
        "    x = self.add_norm(x)\n",
        "    x = self.feed_forward(x)\n",
        "    x = self.add_norm2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZGovLD7n4uG"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model:int, h:int, d_ff:int, vocab_size: int, dropout:float) -> None:\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.masked_attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    self.add_norm = AddNormLayer()\n",
        "    self.cross_attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    self.add_norm2 = AddNormLayer()\n",
        "    self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "    self.add_norm3 = AddNormLayer()\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, trg_mask) -> torch.Tensor:\n",
        "    x = self.masked_attention(x, x, x, trg_mask)\n",
        "    x = self.add_norm(x)\n",
        "    x = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
        "    x = self.add_norm2(x)\n",
        "    x = self.feed_forward(x)\n",
        "    x = self.add_norm3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAcI25CAxkOy"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model:int, h:int, d_ff:int, vocab_size:int, seq_len:int, dropout:float) -> None:\n",
        "    super(Transformer, self).__init__()\n",
        "    self.num_blocks = 6\n",
        "    self.encoder = EncoderBlock(d_model, h, d_ff, dropout)\n",
        "    self.decoder = DecoderBlock(d_model, h, d_ff, vocab_size, dropout)\n",
        "    self.embedding = ImputEmbeddings(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
        "    self.encoder_model = nn.ModuleList([EncoderBlock(d_model, h, d_ff, dropout) for _ in range(self.num_blocks)])\n",
        "    self.decoder_model = nn.ModuleList([DecoderBlock(d_model, h, d_ff, vocab_size, dropout) for _ in range(self.num_blocks)])\n",
        "    self.linear = Linear(d_model, vocab_size)\n",
        "    self.softmax = Softmax(d_model, vocab_size)\n",
        "  def forward(self, src, trg, src_mask, trg_mask) -> torch.Tensor:\n",
        "    src = self.embedding(src)\n",
        "    trg = self.embedding(trg)\n",
        "    src = self.pos_encoding(src)\n",
        "    trg = self.pos_encoding(trg)\n",
        "    for layer in self.encoder_model:\n",
        "      src = layer(src, src_mask)\n",
        "    for layer in self.decoder_model:\n",
        "      trg = layer(trg, src, src_mask, trg_mask)\n",
        "    linear = self.linear(trg)\n",
        "    softmax = self.softmax(linear)\n",
        "    return softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm2-ntf4z1RJ",
        "outputId": "1e90fb74-cb32-4dd5-d6d6-792f1ddb3374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): EncoderBlock(\n",
              "    (attention): MultiHeadAttention(\n",
              "      (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (add_norm): AddNormLayer()\n",
              "    (feed_forward): FeedForward(\n",
              "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "      (relu): ReLU()\n",
              "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (add_norm2): AddNormLayer()\n",
              "  )\n",
              "  (decoder): DecoderBlock(\n",
              "    (masked_attention): MultiHeadAttention(\n",
              "      (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (add_norm): AddNormLayer()\n",
              "    (cross_attention): MultiHeadAttention(\n",
              "      (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (add_norm2): AddNormLayer()\n",
              "    (feed_forward): FeedForward(\n",
              "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "      (relu): ReLU()\n",
              "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (add_norm3): AddNormLayer()\n",
              "  )\n",
              "  (embedding): ImputEmbeddings(\n",
              "    (embed): Embedding(100000, 512)\n",
              "  )\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_model): ModuleList(\n",
              "    (0-5): 6 x EncoderBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (add_norm): AddNormLayer()\n",
              "      (feed_forward): FeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (add_norm2): AddNormLayer()\n",
              "    )\n",
              "  )\n",
              "  (decoder_model): ModuleList(\n",
              "    (0-5): 6 x DecoderBlock(\n",
              "      (masked_attention): MultiHeadAttention(\n",
              "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (add_norm): AddNormLayer()\n",
              "      (cross_attention): MultiHeadAttention(\n",
              "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (add_norm2): AddNormLayer()\n",
              "      (feed_forward): FeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (relu): ReLU()\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (add_norm3): AddNormLayer()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(\n",
              "    (linear): Linear(in_features=512, out_features=100000, bias=True)\n",
              "  )\n",
              "  (softmax): Softmax(\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer = Transformer(d_model = 512, h = 8, d_ff = 2048, vocab_size = 100000, seq_len = 10000, dropout = 0.1)\n",
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvoHUyzV1Wzx",
        "outputId": "8a9e608f-8703-45c8-a648-97828b5e0616"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-71-2bbaf57d373b>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attention_weights = F.softmax(scores)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Çıkış boyutu: torch.Size([2, 10, 1000])\n"
          ]
        }
      ],
      "source": [
        "# Parametreler\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "vocab_size = 1000\n",
        "d_model = 512\n",
        "h = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "# Rastgele input (kelime ID'leri)\n",
        "src = torch.randint(0, vocab_size, (batch_size, seq_len))  # [batch_size, seq_len]\n",
        "trg = torch.randint(0, vocab_size, (batch_size, seq_len))  # [batch_size, seq_len]\n",
        "\n",
        "# Basit mask (şimdilik None ya da sadece 1'ler ile)\n",
        "src_mask = None\n",
        "trg_mask = None\n",
        "\n",
        "# Modeli başlat\n",
        "model = Transformer(d_model, h, d_ff, vocab_size, seq_len, dropout)\n",
        "\n",
        "# Modele giriş ver\n",
        "output = model(src, trg, src_mask, trg_mask)\n",
        "\n",
        "print(\"Çıkış boyutu:\", output.shape)  # [batch_size, seq_len, vocab_size]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
